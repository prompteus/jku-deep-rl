{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TugataiSrEgL"
      },
      "source": [
        "# Setup Notebook (Virtual machine)\n",
        "\n",
        "In this section all required packages will be installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Re86M-UQgoHn",
        "outputId": "d5c03568-d901-42f5-9397-034d31d03213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting setuptools==65.5.1\n",
            "  Downloading setuptools-65.5.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 65.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-65.5.1\n",
            "--2023-05-09 09:36:55--  https://raw.githubusercontent.com/StefOe/colab-pytorch-utils/master/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4519 (4.4K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "utils.py            100%[===================>]   4.41K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-09 09:36:55 (49.3 MB/s) - ‘utils.py’ saved [4519/4519]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 libfontenc1 libpython2-stdlib libxfont2 libxkbfile1 libxtst6\n",
            "  libxxf86dga1 python2 python2-minimal x11-xkb-utils xfonts-base\n",
            "  xfonts-encodings xfonts-utils xserver-common\n",
            "Suggested packages:\n",
            "  python-tk python-numpy libgle3 python2-doc mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 libfontenc1 libpython2-stdlib libxfont2 libxkbfile1 libxtst6\n",
            "  libxxf86dga1 python-opengl python2 python2-minimal x11-utils x11-xkb-utils\n",
            "  xfonts-base xfonts-encodings xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 17 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 8,541 kB of archives.\n",
            "After this operation, 18.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2-minimal amd64 2.7.17-2ubuntu4 [27.5 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-stdlib amd64 2.7.17-2ubuntu4 [7,072 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2 amd64 2.7.17-2ubuntu4 [26.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libfontenc1 amd64 1:1.1.4-0ubuntu1 [14.0 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libxfont2 amd64 1:2.0.3-1 [91.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 libxkbfile1 amd64 1:1.1.0-1 [65.3 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libxtst6 amd64 2:1.2.3-1 [12.8 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu1 [12.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-opengl all 3.1.0+dfsg-2build1 [486 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-utils amd64 7.7+5 [199 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-xkb-utils amd64 7.7+5 [158 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu1 [573 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-utils amd64 1:7.7+6 [91.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 xserver-common all 2:1.20.13-1ubuntu1~20.04.8 [27.2 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 xvfb amd64 2:1.20.13-1ubuntu1~20.04.8 [780 kB]\n",
            "Fetched 8,541 kB in 1s (9,468 kB/s)\n",
            "Selecting previously unselected package python2-minimal.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../python2-minimal_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package libpython2-stdlib:amd64.\n",
            "Preparing to unpack .../libpython2-stdlib_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package python2.\n",
            "(Reading database ... 122547 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python2_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2 (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "Preparing to unpack .../01-freeglut3_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-3) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../02-libfontenc1_1%3a1.1.4-0ubuntu1_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../03-libxfont2_1%3a2.0.3-1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.3-1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../04-libxkbfile1_1%3a1.1.0-1_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../05-libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../06-libxxf86dga1_2%3a1.1.5-0ubuntu1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n",
            "Selecting previously unselected package python-opengl.\n",
            "Preparing to unpack .../07-python-opengl_3.1.0+dfsg-2build1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-2build1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../08-x11-utils_7.7+5_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../09-x11-xkb-utils_7.7+5_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../10-xfonts-encodings_1%3a1.0.5-0ubuntu1_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu1) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../11-xfonts-utils_1%3a7.7+6_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../12-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../13-xserver-common_2%3a1.20.13-1ubuntu1~20.04.8_all.deb ...\n",
            "Unpacking xserver-common (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../14-xvfb_2%3a1.20.13-1ubuntu1~20.04.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-3) ...\n",
            "Setting up libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n",
            "Setting up python2 (2.7.17-2ubuntu4) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.3-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-2build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5) ...\n",
            "Setting up xfonts-utils (1:7.7+6) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up x11-utils (7.7+5) ...\n",
            "Setting up xserver-common (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[accept-rom-license,atari]==0.22.0\n",
            "  Downloading gym-0.22.0.tar.gz (631 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.1/631.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]==0.22.0) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]==0.22.0) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]==0.22.0) (0.0.8)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.4\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.4->gym[accept-rom-license,atari]==0.22.0) (5.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.22.0) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.22.0) (2.27.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.22.0) (8.1.3)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.22.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.22.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.22.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.22.0) (2022.12.7)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.22.0-py3-none-any.whl size=708392 sha256=9f7b57cb4ae2be645c38b5eb1e2496d2f92d07d256f9b5501c27cfd3bacf07a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/e8/e8/6dfbc92a1dcd76c1a5e2bb982750fd6b7e792239f46039e6b1\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446676 sha256=b50a719816eb4cd77cf3f060465a63b5851bb54e17470301ea0a91dc9296030a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: gym, ale-py, AutoROM.accept-rom-license, autorom\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.7.5 autorom-0.4.2 gym-0.22.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx2pytorch\n",
            "  Downloading onnx2pytorch-0.4.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from onnx2pytorch) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from onnx2pytorch) (0.15.1+cu118)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (3.12.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->onnx2pytorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->onnx2pytorch) (16.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.9.0->onnx2pytorch) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.9.0->onnx2pytorch) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->onnx2pytorch) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.9.0->onnx2pytorch) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.9.0->onnx2pytorch) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.9.0->onnx2pytorch) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.9.0->onnx2pytorch) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->onnx2pytorch) (1.3.0)\n",
            "Installing collected packages: onnx, onnx2pytorch\n",
            "Successfully installed onnx-1.14.0 onnx2pytorch-0.4.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym_minigrid==1.1.0\n",
            "  Downloading gym_minigrid-1.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym_minigrid==1.1.0) (1.22.4)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from gym_minigrid==1.1.0) (3.7.1)\n",
            "Requirement already satisfied: gym<=0.26,>=0.22 in /usr/local/lib/python3.10/dist-packages (from gym_minigrid==1.1.0) (0.22.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<=0.26,>=0.22->gym_minigrid==1.1.0) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.26,>=0.22->gym_minigrid==1.1.0) (0.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym_minigrid==1.1.0) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym_minigrid==1.1.0) (8.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym_minigrid==1.1.0) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym_minigrid==1.1.0) (23.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym_minigrid==1.1.0) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym_minigrid==1.1.0) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym_minigrid==1.1.0) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym_minigrid==1.1.0) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gym_minigrid==1.1.0) (1.16.0)\n",
            "Installing collected packages: gym_minigrid\n",
            "Successfully installed gym_minigrid-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install setuptools==65.5.1\n",
        "!wget https://raw.githubusercontent.com/StefOe/colab-pytorch-utils/master/utils.py\n",
        "import utils\n",
        "!apt install -y xvfb x11-utils python-opengl ffmpeg \n",
        "!pip install -U gym[atari,accept-rom-license]==0.22.0\n",
        "!pip install torch\n",
        "# ! pip install -U gym==0.19.0\n",
        "!pip install opencv-python\n",
        "!pip install onnx onnx2pytorch\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gym_minigrid==1.1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMnj8uofocDr",
        "outputId": "9c33c58b-3c2b-457b-e2d8-a0fd32f517ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import time \n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import collections\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import cv2\n",
        "import gym\n",
        "import gym.spaces\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "%matplotlib inline\n",
        "!nvcc --version\n",
        "\n",
        "from matplotlib import animation\n",
        "import seaborn as sns; sns.set()\n",
        "from IPython.display import clear_output, HTML\n",
        "from IPython import display\n",
        "\n",
        "import torch.onnx"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lBjLMoBzrRkq"
      },
      "source": [
        "# Mount your drive and use GPU\n",
        "\n",
        "Here you can mount your GDrive folders (where you will store checkpoints during training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP7nL3c7m6n5",
        "outputId": "dc02432b-d3f4-4b3d-b41b-3a95791b4cfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('/content/gdrive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJyGPhRfQWrV",
        "outputId": "7c60b088-d8bd-431a-8e1c-017420dcf8b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "print(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H8onrKHdtCif"
      },
      "source": [
        "# Replay Buffer\n",
        "\n",
        "The replay buffer stores transitions - state, action, reward, next_state. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-EpB7vkFa202"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self, num_actions, memory_len = 10000):\n",
        "      self.memory_len = memory_len\n",
        "      self.transition = []\n",
        "      self.num_actions = num_actions\n",
        "\n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "      if self.length() > self.memory_len:\n",
        "        self.remove()\n",
        "      self.transition.append(Transition(state, action, reward, next_state, done))\n",
        "\n",
        "  def sample_batch(self, batch_size = 32):\n",
        "      minibatch = random.sample(self.transition, batch_size)\n",
        "      states_mb, a_, reward_mb, next_states_mb, done_mb = map(np.array, zip(*minibatch))\n",
        "\n",
        "      mb_reward = torch.from_numpy(reward_mb).to(device=device, dtype=torch.float32)\n",
        "      mb_done = torch.from_numpy(done_mb.astype(int)).to(device=device)\n",
        "      a_ = a_.astype(int)\n",
        "      a_mb = np.zeros((a_.size, self.num_actions), dtype=np.float32)\n",
        "      a_mb[np.arange(a_.size), a_] = 1\n",
        "      mb_a = torch.from_numpy(a_mb).to(device=device)\n",
        "      return states_mb, mb_a, mb_reward, next_states_mb, mb_done # states will be converted to tensors in forward pass\n",
        "\n",
        "  def length(self):\n",
        "      return len(self.transition)\n",
        "\n",
        "  def remove(self):\n",
        "      self.transition.pop(0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cnQOXbjia2_0"
      },
      "source": [
        "# MINIGRID DQN-Training\n",
        "\n",
        "This section contains the train function for the minigrid environment."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bLEOIJlULd6x"
      },
      "source": [
        "### Minigrid Environment and Policy Network\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "eFHg_Dhjwzi4"
      },
      "outputs": [],
      "source": [
        "# Minigrid Environment\n",
        "from gym_minigrid.wrappers import ImgObsWrapper\n",
        "class ChannelFirst(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        old_shape = env.observation_space.shape\n",
        "        self.observation_space = {}\n",
        "        self.observation_space = gym.spaces.Box(0, 255, shape=(3, 7, 7))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.swapaxes(observation, 2, 0)\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32)\n",
        "\n",
        "class MinigridEmpty5x5ImgObs(gym.Wrapper):\n",
        "    \"\"\"Minigrid with image observations provided by minigrid, partially observable.\"\"\"\n",
        "    def __init__(self):\n",
        "        env = gym.make('MiniGrid-Empty-5x5-v0')\n",
        "        env = ScaledFloatFrame(ChannelFirst(ImgObsWrapper(env)))\n",
        "        super().__init__(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIjUYPNJwzgE",
        "outputId": "84dd1a4f-aab2-453f-913a-96dc656ff522"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[[2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 1., 1., 8., 2.],\n",
              "         [2., 2., 2., 1., 1., 1., 2.],\n",
              "         [2., 2., 2., 1., 1., 1., 2.]],\n",
              " \n",
              "        [[5., 5., 5., 5., 5., 5., 5.],\n",
              "         [5., 5., 5., 5., 5., 5., 5.],\n",
              "         [5., 5., 5., 5., 5., 5., 5.],\n",
              "         [5., 5., 5., 5., 5., 5., 5.],\n",
              "         [5., 5., 5., 0., 0., 1., 5.],\n",
              "         [5., 5., 5., 0., 0., 0., 5.],\n",
              "         [5., 5., 5., 0., 0., 0., 5.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.]]], dtype=float32),\n",
              " (3, 7, 7),\n",
              " Discrete(7))"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = MinigridEmpty5x5ImgObs()\n",
        "obs = env.reset()\n",
        "obs, obs.shape, env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "QVuKE2l-wzdf"
      },
      "outputs": [],
      "source": [
        "class MlpMinigridPolicy(nn.Module):\n",
        "    def __init__(self, num_actions=7):\n",
        "        super().__init__()\n",
        "        self.num_actions = num_actions\n",
        "        self.fc = nn.Sequential(nn.Flatten(), \n",
        "                                nn.Linear(3*7**2, 256), nn.ReLU(),\n",
        "                                nn.Linear(256, 256), nn.ReLU(),\n",
        "                                nn.Linear(256, 64), nn.ReLU(),\n",
        "                                nn.Linear(64, num_actions))\n",
        "    def forward(self, x):\n",
        "        # print(x, type(x))\n",
        "        x = torch.tensor(x, dtype=torch.float32, device=device)\n",
        "        if len(x.size()) == 3:\n",
        "          x = x.unsqueeze(dim=0)\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWtwq6OfwzTD",
        "outputId": "b0acaca3-ad07-4199-9192-2635b4d3fdbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.2311, -0.0256, -0.1468, -0.3663,  0.0537, -0.1917,  0.0853]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp_minigrid_policy = MlpMinigridPolicy().to(device)\n",
        "mlp_minigrid_policy(obs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lv3rBDMva2_7"
      },
      "source": [
        "### Training utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rfdwKpEfa2_8"
      },
      "outputs": [],
      "source": [
        "def plot(frame_idx, rewards, losses):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
        "    plt.plot(rewards)\n",
        "    plt.subplot(132)\n",
        "    plt.title('loss')\n",
        "    plt.plot(losses)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2e2kh79ta2_9"
      },
      "outputs": [],
      "source": [
        "# Update Target network\n",
        "def soft_update(local_model, target_model, tau):\n",
        "    \"\"\"Soft update model parameters.\n",
        "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "    Params\n",
        "    ======\n",
        "        local_model (PyTorch model): weights will be copied from\n",
        "        target_model (PyTorch model): weights will be copied to\n",
        "        tau (float): interpolation parameter \n",
        "    \"\"\"\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "n042G7Gna2_-"
      },
      "outputs": [],
      "source": [
        "def set_seed(env, seed=None):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        env.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            torch.cuda.manual_seed_all(seed)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1fX8mCHQa2__"
      },
      "source": [
        "### DQN Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiQvC57wa2__",
        "outputId": "0b5aef2e-a08f-4a2d-e0db-7b74a8bf1d2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f031a30aef0>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_episodes = 1000 # number of episodes to run the algorithm -> the algorithm should reach a score of about 0.9 within 1000 episodes\n",
        "buffer_size = 100_000 # size of the buffer to use\n",
        "epsilon = 1.0 # initial probablity of selecting random action a, annealed over time\n",
        "timesteps = 0 # counter for number of frames\n",
        "minibatch_size = 128 # size of the minibatch sampled\n",
        "gamma = 0.99 # discount factor\n",
        "eval_episode = 100\n",
        "num_eval = 10\n",
        "tau = 1e-3 \n",
        "learning_rate = 0.0001\n",
        "update_after = 2000 # update after num time steps\n",
        "epsilon_decay = 100_000 # decay epsilon in 100.000 timesteps\n",
        "epsilon_ub = 1.0\n",
        "epsilon_lb = 0.3\n",
        "# set seed\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEjbqo3ea3AA",
        "outputId": "6c451317-bfbc-4e70-efdf-ceba644bcb6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7 (3, 7, 7)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([[[2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 1., 1., 8., 2.],\n",
              "         [2., 2., 2., 1., 1., 1., 2.],\n",
              "         [2., 2., 2., 1., 1., 1., 2.]],\n",
              " \n",
              "        [[5., 5., 5., 5., 5., 5., 5.],\n",
              "         [5., 5., 5., 5., 5., 5., 5.],\n",
              "         [5., 5., 5., 5., 5., 5., 5.],\n",
              "         [5., 5., 5., 5., 5., 5., 5.],\n",
              "         [5., 5., 5., 0., 0., 1., 5.],\n",
              "         [5., 5., 5., 0., 0., 0., 5.],\n",
              "         [5., 5., 5., 0., 0., 0., 5.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.]]], dtype=float32),\n",
              " (3, 7, 7),\n",
              " Discrete(7))"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize environment\n",
        "env = MinigridEmpty5x5ImgObs()\n",
        "\n",
        "num_actions = env.action_space.n\n",
        "state_space = env.observation_space.shape\n",
        "print(num_actions, state_space)\n",
        "obs, obs.shape, env.action_space"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2HI0DN4la3AA"
      },
      "source": [
        "### Load model (if available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ48jyp2a3AB",
        "outputId": "21306529-4e78-4b5a-faa4-c7407282e10b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne7v8WrBa3AB",
        "outputId": "61750bda-6a39-457d-b7e5-5539f60528ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model checkpoints will be saved to: /content/gdrive/MyDrive/colab-drive/minigrid-model-2023_05_09-09_51_28.p\n"
          ]
        }
      ],
      "source": [
        "# specify load path if available:\n",
        "\n",
        "# load_path = '/content/gdrive/MyDrive/colab-drive/minigrid-model-2022_03_28-10_01_18.p'\n",
        "\n",
        "load_path = '' # otherwise start with randomly initialized agent\n",
        "\n",
        "# save the current model state\n",
        "save_path = f\"/content/gdrive/MyDrive/colab-drive/minigrid-model-{datetime.now().strftime('%Y_%m_%d-%H_%M_%S')}.p\"\n",
        "print(f'model checkpoints will be saved to: {save_path}')\n",
        "\n",
        "def load_checkpoint(checkpoint_path='', device=device):\n",
        "  dqn = MlpMinigridPolicy(num_actions=num_actions).to(device=device)\n",
        "  dqn_target = MlpMinigridPolicy(num_actions=num_actions).to(device=device)\n",
        "  timesteps = 0\n",
        "\n",
        "  if checkpoint_path:\n",
        "    print(f'Loading checkpoint {checkpoint_path}')\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location=device)\n",
        "    model_params = checkpoint_dict['model_params']\n",
        "    timesteps = checkpoint_dict['timesteps'] # environment steps\n",
        "\n",
        "    dqn.load_state_dict(model_params) # makes a copy of model_params\n",
        "    dqn_target.load_state_dict(model_params)\n",
        "  else:\n",
        "    print(f'Starting training from scratch.')\n",
        "\n",
        "  return dqn, dqn_target, timesteps\n",
        "\n",
        "def store_checkpoint(checkpoint_path, dqn_net, timesteps):\n",
        "  checkpoint_dict = {'model_params':dqn_net.state_dict(), 'timesteps': timesteps}\n",
        "  torch.save(checkpoint_dict, checkpoint_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CCtG6K2Fa3AC"
      },
      "source": [
        "### DQN training loop\n",
        "\n",
        "**TODO**: Implement DQN algorithm (lines highlighted with TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIbjVRHKa3AD"
      },
      "outputs": [],
      "source": [
        "from torch.serialization import load\n",
        "\n",
        "# Train the agent using DQN for Pong\n",
        "returns = []\n",
        "returns_50 = deque(maxlen=50)\n",
        "losses = []\n",
        "buffer = ReplayBuffer(num_actions=num_actions, memory_len=buffer_size)\n",
        "\n",
        "dqn, dqn_target, timesteps = load_checkpoint(load_path)\n",
        "\n",
        "optimizer = optim.Adam(dqn.parameters(), lr=learning_rate)\n",
        "mse = torch.nn.MSELoss()\n",
        "state = env.reset()\n",
        "for i in range(num_episodes):\n",
        "  ret = 0\n",
        "  done = False\n",
        "  while not done:\n",
        "    # Decay epsilon\n",
        "    epsilon = max(epsilon_lb, epsilon_ub - timesteps/ epsilon_decay)\n",
        "    # action selection\n",
        "    if np.random.choice([0,1], p=[1-epsilon,epsilon]) == 1:\n",
        "      a = np.random.randint(low=0, high=num_actions, size=1)[0]\n",
        "    else:\n",
        "      net_out = dqn(state).detach().cpu().numpy()\n",
        "      a = np.argmax(net_out)\n",
        "    next_state, r, done, info = env.step(a)\n",
        "    ret = ret + r\n",
        "    # TODO: store transition in replay buffer \n",
        "    # ....\n",
        "    state = next_state\n",
        "    timesteps = timesteps + 1\n",
        "\n",
        "    # update policy using temporal difference\n",
        "    if buffer.length() > minibatch_size and buffer.length() > update_after:\n",
        "      optimizer.zero_grad()\n",
        "      # TODO: Sample a minibatch randomly\n",
        "      # ....      \n",
        "      # TODO: Compute q values for states\n",
        "      # ....\n",
        "      # ....\n",
        "      # TODO: compute the targets for training\n",
        "      # ....\n",
        "      # TODO: compute the predictions for training\n",
        "      # ....\n",
        "      # Update loss: mse = mean squared error\n",
        "      loss = mse(predictions, targets) \n",
        "      loss.backward(retain_graph=False)\n",
        "      optimizer.step()\n",
        "      losses.append(loss.item())\n",
        " \n",
        "      # Update target network\n",
        "      soft_update(dqn, dqn_target, tau)\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "      print(f\"Episode: \\t{i}\\t{ret}\\t{datetime.now().strftime('%Y_%m_%d-%H_%M_%S')}\")\n",
        "      break\n",
        "  returns.append(ret)\n",
        "  returns_50.append(ret)\n",
        "  if i % 50 == 0:\n",
        "    store_checkpoint(checkpoint_path=save_path, dqn_net=dqn, timesteps=timesteps)\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i, np.mean(returns_50)))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HnGHZVCa3AD"
      },
      "outputs": [],
      "source": [
        "plot(timesteps, returns, losses)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bQw2KKKYa3AE"
      },
      "source": [
        "### Record video of your Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJM9hM1Oa3AE"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from gym.wrappers import Monitor\n",
        "# start virtual display\n",
        "# from pyvirtualdisplay import Display\n",
        "# pydisplay = Display(visible=0, size=(640, 480))\n",
        "# pydisplay.start()\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env_video(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiUznl8wa3AF"
      },
      "outputs": [],
      "source": [
        "video_env = wrap_env_video(env)\n",
        "state = video_env.reset()\n",
        "ret = 0\n",
        "done = False\n",
        "while not done:\n",
        "  state_tensor = torch.tensor(state, dtype=torch.float, device=device)\n",
        "  # state_tensor = state_tensor.unsqueeze(dim=0)\n",
        "  q_actions = dqn(state_tensor).detach().cpu().numpy()\n",
        "  action = np.argmax(q_actions)\n",
        "  state, r, done, _ = video_env.step(action)\n",
        "  ret += r\n",
        "video_env.close()\n",
        "print(f\"Episode Return: {ret}\")\n",
        "show_video() # if not shown properly, just download the video in the \"video\" folder\n",
        "# if the agent is not moving or not reaching the goal, restart training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jqYK_XTz3RPQ"
      },
      "source": [
        "# PONG DQN-Training\n",
        "\n",
        "This section contains the train function for the Pong environment."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K47IPM-GsAsw"
      },
      "source": [
        "### Atari Wrappers\n",
        "\n",
        "In this section all [OpenAI Gym wrappers](https://www.gymlibrary.ml/pages/api/#wrappers) used in the original [DQN-Atari paper](https://hallab.cs.dal.ca/images/0/00/Minh2015.pdf) are defined.\n",
        "\n",
        "note: this code cell is mainly a copy from https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "8fncR2vpJK_C"
      },
      "outputs": [],
      "source": [
        "from gym import spaces\n",
        "import cv2\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1)\n",
        "\n",
        "#            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done  = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip       = skip\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = spaces.Box(low=0, high=255,\n",
        "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        return frame[:, :, None]\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8)\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "        self._out = None\n",
        "\n",
        "    def _force(self):\n",
        "        if self._out is None:\n",
        "            self._out = np.concatenate(self._frames, axis=2)\n",
        "            self._frames = None\n",
        "        return self._out\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = self._force()\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._force())\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._force()[i]\n",
        "\n",
        "def make_atari(env_id):\n",
        "    env = gym.make(env_id)\n",
        "    assert 'NoFrameskip' in env.spec.id\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    return env\n",
        "\n",
        "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=True, scale=False):\n",
        "    \"\"\"Configure environment for DeepMind-style Atari.\n",
        "    \"\"\"\n",
        "    if episode_life:\n",
        "        env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    if scale:\n",
        "        env = ScaledFloatFrame(env)\n",
        "    if clip_rewards:\n",
        "        env = ClipRewardEnv(env)\n",
        "    if frame_stack:\n",
        "        env = FrameStack(env, 4)\n",
        "    return env\n",
        "\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Image shape to num_channels x weight x height\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.swapaxes(observation, 2, 0)\n",
        "    \n",
        "\n",
        "def wrap_pytorch(env):\n",
        "    return ImageToPyTorch(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "_bYqXnnIG_6O"
      },
      "outputs": [],
      "source": [
        "def test_environment(env, agent=None):\n",
        "    state = env.reset()\n",
        "\n",
        "    prev_screen = env.render(mode='rgb_array')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(prev_screen)\n",
        "\n",
        "    for i in range(200):\n",
        "        env.render()\n",
        "\n",
        "        if agent is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action, _ = agent(state)\n",
        "            action = action.item()\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            env.reset()\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GzT6K2aP_iRL"
      },
      "source": [
        "### DQN Atari Network\n",
        "\n",
        "Here, the DQN network class is defined. \n",
        "It has a couple of CNN layers (as the observations are images - CNN layers almost always work best for image inputs) followed by linear layers with the number of outputs.\n",
        "See the [DQN Atari paper](https://hallab.cs.dal.ca/images/0/00/Minh2015.pdf) for details.\n",
        "\n",
        "`forward` should give for a state / observation the expected return from t to T for every possible action, such that we can use argmax on the outputs later to determine the action.\n",
        "\n",
        "\n",
        "**TODO:** Implement network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "g0vAAN8opzeg"
      },
      "outputs": [],
      "source": [
        "class DQNNetwork_atari(nn.Module):\n",
        "\n",
        "    def __init__(self, num_actions = 4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        # Network Architecture\n",
        "        # 4 input image channels, 16 output channels, 8x8 square convolution, stride=4, with relus\n",
        "        # 16 input channels, 32 output channels, 4x4 square convolution, stride=2, with relus\n",
        "        # 256 units linear layer, with relus\n",
        "        self.features = nn.Sequential(\n",
        "            # TODO\n",
        "          )\n",
        "          \n",
        "        self.fc = nn.Sequential(\n",
        "            # TODO\n",
        "          )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tensor(x, dtype=torch.float).to(device=device)\n",
        "        if len(x.size()) == 3:\n",
        "          x = x.unsqueeze(dim=0)\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jpl25jrxobUy"
      },
      "source": [
        "### Keep notebook alive\n",
        "Don't forget to run the following function on the console of the virtal machine as soon as you have started.\n",
        "(For details we refer to the technical tutorial)\n",
        "1. Press `F12` or `Ctrl+Shift+I` and go to Console and paste the following code:\n",
        "``` javascript\n",
        "function random(min,max) { return Math.floor((Math.random())*(max-min+1))+min; }\n",
        "function ClickConnect(){\n",
        "console.log(\"Try keepalive\");\n",
        "cells = document.querySelectorAll(\"div.main-content > div.codecell-input-output > div.output\");\n",
        "idx = random(0, cells.length);\n",
        "cells[idx].click();\n",
        "recapt = document.querySelector(\".recaptcha-checkbox\");\n",
        "if (recapt == null) {\n",
        "recapt.click();\n",
        "}\n",
        "}\n",
        "var keepAlive = setInterval(ClickConnect,6000)\n",
        "```\n",
        "2. Stop `keepAlive` when you continue with on the notebook.\n",
        "``` javascript\n",
        "clearInterval(keepAlive)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "QO7RPnP5Hnay"
      },
      "outputs": [],
      "source": [
        "def plot(frame_idx, rewards, losses):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
        "    plt.plot(rewards)\n",
        "    plt.subplot(132)\n",
        "    plt.title('loss')\n",
        "    plt.plot(losses)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Lw8KflTIk4_2"
      },
      "outputs": [],
      "source": [
        "# Update Target network\n",
        "def soft_update(local_model, target_model, tau):\n",
        "    \"\"\"Soft update model parameters.\n",
        "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "    Params\n",
        "    ======\n",
        "        local_model (PyTorch model): weights will be copied from\n",
        "        target_model (PyTorch model): weights will be copied to\n",
        "        tau (float): interpolation parameter \n",
        "    \"\"\"\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "xJm0gYFn5OQ_"
      },
      "outputs": [],
      "source": [
        "def set_seed(env, seed=None):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        env.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            torch.cuda.manual_seed_all(seed)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "inAh01sSChwG"
      },
      "source": [
        "### DQN Hyperparameters\n",
        "\n",
        "**TODO**: Try out other hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wei3Fo1bbXa4",
        "outputId": "bc8db5d4-70c4-4f08-8708-634065dbc77f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f031a30aef0>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env_id = \"PongNoFrameskip-v4\"\n",
        "num_episodes = 500 #1000 #600 # number of episodes to run the algorithm\n",
        "buffer_size = 1e5 #10 ** 5 * 3 # size of the buffer to use\n",
        "epsilon = 1.0 # initial probablity of selecting random action a, annealed over time\n",
        "timesteps = 0 # counter for number of frames\n",
        "minibatch_size = 128 # size of the minibatch sampled\n",
        "gamma = 0.99 # discount factor\n",
        "eval_episode = 100\n",
        "num_eval = 10\n",
        "tau = 1e-3 \n",
        "learning_rate = 0.00001\n",
        "update_after = 2000 # update after num time steps\n",
        "epsilon_decay = 10**5\n",
        "epsilon_ub = 1.0\n",
        "epsilon_lb = 0.02\n",
        "# set seed\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz2Op79LbiAB",
        "outputId": "9abfc5c8-50c6-48b2-eb9e-1889add1c235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6 (4, 84, 84) <bound method ClipRewardEnv.reward of <ClipRewardEnv<WarpFrame<FireResetEnv<EpisodicLifeEnv<MaxAndSkipEnv<NoopResetEnv<TimeLimit<OrderEnforcing<AtariEnv<PongNoFrameskip-v4>>>>>>>>>>>\n"
          ]
        }
      ],
      "source": [
        "# Initialize environment\n",
        "env_id = \"PongNoFrameskip-v4\"\n",
        "env = make_atari(env_id)\n",
        "env = wrap_deepmind(env)\n",
        "env = wrap_pytorch(env)\n",
        "\n",
        "num_actions = env.action_space.n\n",
        "state_space = env.observation_space.shape\n",
        "print(num_actions, state_space, env.reward)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "11By362woNnO"
      },
      "source": [
        "### Load model (if available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F537WjT64aqa",
        "outputId": "e2a4c4f0-0510-4370-b054-84c3aa9001ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln2NOMZygsjY",
        "outputId": "22d9674e-beb6-4085-8e90-aee5c2e238e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model checkpoints will be saved to: /content/gdrive/MyDrive/colab-drive/model-2023_05_09-09_53_24.p\n"
          ]
        }
      ],
      "source": [
        "# specify load path if available:\n",
        "\n",
        "load_path = '/content/gdrive/MyDrive/colab-drive/model-2022_03_28-10_01_18.p'\n",
        "\n",
        "# load_path = '' # otherwise start with randomly initialized agent\n",
        "\n",
        "# save the current model state\n",
        "save_path = f\"/content/gdrive/MyDrive/colab-drive/model-{datetime.now().strftime('%Y_%m_%d-%H_%M_%S')}.p\"\n",
        "print(f'model checkpoints will be saved to: {save_path}')\n",
        "\n",
        "def load_checkpoint(checkpoint_path='', device=device):\n",
        "  dqn = DQNNetwork_atari(num_actions=num_actions).to(device=device)\n",
        "  dqn_target = DQNNetwork_atari(num_actions=num_actions).to(device=device)\n",
        "  timesteps = 0\n",
        "\n",
        "  if checkpoint_path:\n",
        "    print(f'Loading checkpoint {checkpoint_path}')\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location=device)\n",
        "    model_params = checkpoint_dict['model_params']\n",
        "    timesteps = checkpoint_dict['timesteps'] # environment steps\n",
        "\n",
        "    dqn.load_state_dict(model_params) # makes a copy of model_params\n",
        "    dqn_target.load_state_dict(model_params)\n",
        "  else:\n",
        "    print(f'Starting training from scratch.')\n",
        "\n",
        "  return dqn, dqn_target, timesteps\n",
        "\n",
        "def store_checkpoint(checkpoint_path, dqn_net, timesteps):\n",
        "  checkpoint_dict = {'model_params':dqn_net.state_dict(), 'timesteps': timesteps}\n",
        "  torch.save(checkpoint_dict, checkpoint_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1ymrNE0p9iK2"
      },
      "source": [
        "### DQN training loop\n",
        "\n",
        "**TODO**: Implement DQN algorithm (lines highlighted with TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jzQ6Q8rYVKw"
      },
      "outputs": [],
      "source": [
        "from torch.serialization import load\n",
        "\n",
        "# Train the agent using DQN for Pong\n",
        "returns = []\n",
        "returns_50 = deque(maxlen=50)\n",
        "losses = []\n",
        "buffer = ReplayBuffer(num_actions=num_actions, memory_len=buffer_size)\n",
        "\n",
        "dqn, dqn_target, timesteps = load_checkpoint(load_path)\n",
        "\n",
        "optimizer = optim.Adam(dqn.parameters(), lr=learning_rate)\n",
        "mse = torch.nn.MSELoss()\n",
        "state = env.reset()\n",
        "for i in range(num_episodes):\n",
        "  ret = 0\n",
        "  done = False\n",
        "  while not done:\n",
        "    # Decay epsilon\n",
        "    epsilon = max(epsilon_lb, epsilon_ub - timesteps/ epsilon_decay)\n",
        "    # action selection\n",
        "    if np.random.choice([0,1], p=[1-epsilon,epsilon]) == 1:\n",
        "      a = np.random.randint(low=0, high=num_actions, size=1)[0]\n",
        "    else:\n",
        "      net_out = dqn(state).detach().cpu().numpy()\n",
        "      a = np.argmax(net_out)\n",
        "    next_state, r, done, info = env.step(a)\n",
        "    ret = ret + r\n",
        "    # TODO: store transition in replay buffer \n",
        "    # ....\n",
        "    state = next_state\n",
        "    timesteps = timesteps + 1\n",
        "\n",
        "    # update policy using temporal difference\n",
        "    if buffer.length() > minibatch_size and buffer.length() > update_after:\n",
        "      optimizer.zero_grad()\n",
        "      # TODO: Sample a minibatch randomly\n",
        "      # ....      \n",
        "      # TODO: Compute q values for states\n",
        "      # ....\n",
        "      # ....\n",
        "      # TODO: compute the targets for training\n",
        "      # ....\n",
        "      # TODO: compute the predictions for training\n",
        "      # ....\n",
        "      # Update loss: mse = mean squared error\n",
        "      loss = mse(predictions, targets) \n",
        "      loss.backward(retain_graph=False)\n",
        "      optimizer.step()\n",
        "      losses.append(loss.item())\n",
        " \n",
        "      # Update target network\n",
        "      soft_update(dqn, dqn_target, tau)\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "      print(f\"Episode: \\t{i}\\t{ret}\\t{datetime.now().strftime('%Y_%m_%d-%H_%M_%S')}\")\n",
        "      break\n",
        "  returns.append(ret)\n",
        "  returns_50.append(ret)\n",
        "  store_checkpoint(checkpoint_path=save_path, dqn_net=dqn, timesteps=timesteps)\n",
        "  if i % 50 == 0:\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i, np.mean(returns_50)))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzbwR2RoGWLf"
      },
      "outputs": [],
      "source": [
        "plot(timesteps, returns, losses)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q21cFppy9xnW"
      },
      "source": [
        "### Prepare trained model for submission\n",
        "\n",
        "Export to onnx, test final performance, etc.\n",
        "\n",
        "**TODO**: Export your model. It should be enough to insert the correct path to your checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEzUFFz5jwNa"
      },
      "outputs": [],
      "source": [
        "# load model from checkpoint\n",
        "checkpoint_path = '/content/gdrive/MyDrive/colab-drive/model-2022_03_28-10_01_18.p' # TODO insert your path here\n",
        "dqn, _, _ = load_checkpoint(checkpoint_path, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83LGjWQWklDG"
      },
      "outputs": [],
      "source": [
        "state = env.reset()\n",
        "state_for_onnx = state[np.newaxis, :] # set batch-size=1 - important for evaluation on challenge server\n",
        "print(state_for_onnx.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB_ks0Q7KYPH"
      },
      "outputs": [],
      "source": [
        "# save model parameters for submitting solution using the following line of code\n",
        "onnx_path = f\"/content/gdrive/MyDrive/colab-drive/submission.onnx\"\n",
        "torch.onnx.export(dqn, # model\n",
        "                  torch.tensor(state_for_onnx, dtype=torch.float), # example model input\n",
        "                  onnx_path, # file path\n",
        "                  export_params=True, # save trained parameters\n",
        "                  opset_version=10,\n",
        "                  do_constant_folding=True)\n",
        "                  # input_names=['input'],\n",
        "                  # output_names=['output'],\n",
        "                  # dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\n",
        "                  #               'output' : {0 : 'batch_size'}}) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEy80CJblrWN"
      },
      "outputs": [],
      "source": [
        "# Check your model using onnx\n",
        "import onnx\n",
        "onnx_model = onnx.load(onnx_path)\n",
        "onnx.checker.check_model(onnx_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOTrnK0dmgDL"
      },
      "outputs": [],
      "source": [
        "from onnx2pytorch import ConvertModel\n",
        "dqn = ConvertModel(onnx.load(onnx_path))\n",
        "dqn = dqn.to(device)\n",
        "dqn.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCFmc2Dso2fa"
      },
      "outputs": [],
      "source": [
        "# test on the env using onnx model\n",
        "return_ = []\n",
        "for i in range(5):\n",
        "  state = env.reset()\n",
        "  ret = 0\n",
        "  done = False\n",
        "  while not done:\n",
        "    state_tensor = torch.tensor(state, dtype=torch.float, device=device)\n",
        "    state_tensor = state_tensor.unsqueeze(dim=0)\n",
        "    q_actions = dqn(state_tensor).detach().cpu().numpy()\n",
        "    action = np.argmax(q_actions)\n",
        "    state, r, done, _ = env.step(action)\n",
        "    ret += r\n",
        "  print(ret)\n",
        "  return_.append(ret)\n",
        "\n",
        "print(\"Average Return:\", np.mean(return_))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IJqPBNT8mwOc"
      },
      "source": [
        "### Record video of your Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JirC7G0fk9Fs"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from gym.wrappers import Monitor\n",
        "# start virtual display\n",
        "# from pyvirtualdisplay import Display\n",
        "# pydisplay = Display(visible=0, size=(640, 480))\n",
        "# pydisplay.start()\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env_video(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0grHe9nJo7mJ"
      },
      "outputs": [],
      "source": [
        "video_env = wrap_env_video(env)\n",
        "state = video_env.reset()\n",
        "ret = 0\n",
        "done = False\n",
        "while not done:\n",
        "  state_tensor = torch.tensor(state, dtype=torch.float, device=device)\n",
        "  state_tensor = state_tensor.unsqueeze(dim=0)\n",
        "  q_actions = dqn(state_tensor).detach().cpu().numpy()\n",
        "  action = np.argmax(q_actions)\n",
        "  state, r, done, _ = video_env.step(action)\n",
        "  ret += r\n",
        "video_env.close()\n",
        "print(f\"Episode Return: {ret}\")\n",
        "show_video() # if not shown properly, just download the video in the \"video\" folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hf71UiPlpKz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
