{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import abc\n",
    "import random\n",
    "import itertools\n",
    "import functools\n",
    "import collections\n",
    "import pathlib\n",
    "import warnings\n",
    "from typing import NamedTuple, Any, Iterable, Callable\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import lightning\n",
    "import lightning.pytorch\n",
    "import lightning.pytorch.loggers\n",
    "import lightning.pytorch.callbacks\n",
    "import gym\n",
    "import gym.spaces\n",
    "import lovely_tensors\n",
    "import lovely_numpy\n",
    "import wandb\n",
    "import wandb.wandb_run\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from envs import make_env_minigrid, make_env_pong\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "def lovely(x):\n",
    "    \"summarizes important tensor properties\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return lovely_numpy.lovely(x)\n",
    "    return lovely_tensors.lovely(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    state: torch.Tensor | np.ndarray\n",
    "    action: torch.Tensor | np.ndarray\n",
    "    reward: torch.Tensor | np.ndarray\n",
    "    next_state: torch.Tensor | np.ndarray\n",
    "    done: torch.Tensor | np.ndarray | bool\n",
    "\n",
    "\n",
    "class ReplayBuffer(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, capacity: int, seed = None):\n",
    "        self.capacity = capacity\n",
    "        self.buffer: list[Transition] = []\n",
    "        self.seed = seed\n",
    "        self.rng = random.Random(seed)\n",
    "    \n",
    "    def __next__(self):\n",
    "        transition = self.rng.choice(self.buffer)\n",
    "        transition = Transition(\n",
    "            state = torch.tensor(transition.state, dtype=torch.float32),\n",
    "            action = torch.tensor(transition.action, dtype=torch.int64),\n",
    "            reward = torch.tensor(transition.reward, dtype=torch.float32),\n",
    "            next_state = torch.tensor(transition.next_state, dtype=torch.float32),\n",
    "            done = torch.tensor(transition.done.item(), dtype=torch.bool),\n",
    "        )\n",
    "        return transition\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def add(self, t: Transition):\n",
    "        self.buffer.append(t)\n",
    "        if len(self.buffer) > self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "class MlpQNet(torch.nn.Module):\n",
    "    def __init__(self, num_actions: int):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(start_dim=1), \n",
    "            torch.nn.Linear(3*7**2, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        if isinstance(x, np.ndarray):\n",
    "            param = next(self.parameters())\n",
    "            x = torch.tensor(x).to(param)\n",
    "\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def set_submodule(module: torch.nn.Module, submodule_path: str, submodule: torch.nn.Module) -> None:\n",
    "    *parent_path, attr_name = submodule_path.split(\".\")\n",
    "    parent = module\n",
    "    for child in parent_path:\n",
    "        parent = getattr(parent, child)\n",
    "    setattr(parent, attr_name, submodule)\n",
    "\n",
    "\n",
    "# Copied from my previous homework\n",
    "class VisionQNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Policy network is compatible with any CNN architecture as long as:\n",
    "    1) One of the conv layers determines how many channels it can process\n",
    "    2) One of the linear layers determines the number of output neurons\n",
    "    \n",
    "    This should be enough to support all standard architectures\n",
    "\n",
    "    >>> policy_nn = PolicyNetwork(\n",
    "    ...     actions=5,\n",
    "    ...     architecture=\"efficientnet_b0\",\n",
    "    ...     input_conv=\"features.0.0\",\n",
    "    ...     output_lin=\"classifier.1\",\n",
    "    ...     pretrained_weights=None, # \"IMAGENET1K_V1\"\n",
    "    ... )\n",
    "    >>> inputs = torch.rand(size=(16, 1, 64, 64))\n",
    "    >>> policy_nn(inputs).shape\n",
    "    torch.Size([16, 5])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        actions: int | dict[int, str],\n",
    "        architecture: str,\n",
    "        input_conv: str,\n",
    "        output_lin: str,\n",
    "        input_channels: int,\n",
    "        pretrained_weights = None,\n",
    "        seed: int = 42,\n",
    "    ) -> None:\n",
    "        super(VisionQNet, self).__init__()\n",
    "        if isinstance(actions, int):\n",
    "            self.n_units_out = actions\n",
    "            self.actions = {i: f\"action_{i}\" for i in range(actions)}\n",
    "        else:\n",
    "            self.n_units_out = len(actions)\n",
    "            self.actions = actions\n",
    "        self.architecture = architecture\n",
    "        self.input_conv = input_conv\n",
    "        self.output_lin = output_lin\n",
    "        self.input_channels = input_channels\n",
    "        self.nn = torchvision.models.get_model(self.architecture)\n",
    "        self._patch_input_shape()\n",
    "        self._patch_output_shape()\n",
    "        self.init(pretrained_weights, seed)\n",
    "\n",
    "    def init(self, pretrained_weights = None, seed: int = None) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the model parameters, either randomly or with pretrained weights.\n",
    "        Patches the architecture to have desired input and output shape.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            torch.random.manual_seed(seed)\n",
    "\n",
    "        state_dict = torchvision.models.get_model(self.architecture, weights=pretrained_weights).state_dict()\n",
    "\n",
    "        for key in list(state_dict.keys()):\n",
    "            if key.startswith(self.input_conv) or key.startswith(self.output_lin):\n",
    "                state_dict.pop(key)\n",
    "\n",
    "        incompatible = self.nn.load_state_dict(state_dict, strict=False)\n",
    "        if len(incompatible.unexpected_keys) != 0:\n",
    "            raise ValueError(f\"Unexpected additional keys in pretrained weights: {incompatible.unexpected_keys}\")\n",
    "        \n",
    "        for key in incompatible.missing_keys:\n",
    "            assert isinstance(key, str)\n",
    "            if key.startswith(self.input_conv) or key.startswith(self.output_lin):\n",
    "                continue\n",
    "            raise ValueError(f\"Unexpected missing key in pretrained weights: {key}\")\n",
    "\n",
    "        del state_dict\n",
    "\n",
    "        in_out_params = itertools.chain(\n",
    "            self.nn.get_submodule(self.input_conv).parameters(),\n",
    "            self.nn.get_submodule(self.output_lin).parameters(),\n",
    "        )\n",
    "\n",
    "        for param in in_out_params:\n",
    "            torch.nn.init.trunc_normal_(param, mean=0, std=1e-4, a=-0.01, b=0.01)            \n",
    "\n",
    "    def _patch_input_shape(self) -> None:\n",
    "        \"\"\"\n",
    "        Make the architecture accept a single (grayscale) channel\n",
    "        \"\"\"\n",
    "        old_in_conv = self.nn.get_submodule(self.input_conv)\n",
    "        assert isinstance(old_in_conv, torch.nn.Conv2d)\n",
    "\n",
    "        if old_in_conv.in_channels == 1:\n",
    "            return\n",
    "\n",
    "        new_in_conv = torch.nn.Conv2d(\n",
    "            in_channels = self.input_channels, \n",
    "            out_channels = old_in_conv.out_channels,\n",
    "            kernel_size = old_in_conv.kernel_size,\n",
    "            stride = old_in_conv.stride,\n",
    "            padding = old_in_conv.padding,\n",
    "            bias = old_in_conv.bias is not None,\n",
    "        ).to(\n",
    "            old_in_conv.weight\n",
    "        )\n",
    "        set_submodule(self.nn, self.input_conv, new_in_conv)\n",
    "\n",
    "    def _patch_output_shape(self) -> None:\n",
    "        \"\"\"\n",
    "        Make the architecture output the correct shape\n",
    "        \"\"\"\n",
    "        old_out_lin = self.nn.get_submodule(self.output_lin)\n",
    "        assert isinstance(old_out_lin, torch.nn.Linear)\n",
    "        \n",
    "        if old_out_lin.out_features == self.n_units_out:\n",
    "            return\n",
    "\n",
    "        new_out_lin = torch.nn.Linear(\n",
    "            in_features = old_out_lin.in_features,\n",
    "            out_features = self.n_units_out,\n",
    "            bias = old_out_lin.bias is not None,\n",
    "        ).to(\n",
    "            old_out_lin.weight\n",
    "        )\n",
    "        set_submodule(self.nn, self.output_lin, new_out_lin)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisionQNet(5, \"mobilenet_v3_small\", \"features.0.0\", \"classifier.3\", input_channels=1)\n",
    "None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_minigrid = make_env_minigrid()\n",
    "state = env_minigrid.reset()\n",
    "print(\"obs:\", lovely(state))\n",
    "print(\"metadata:\", env_minigrid.metadata)\n",
    "print(\"action_space:\", env_minigrid.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_pong = make_env_pong()\n",
    "state = env_pong.reset()\n",
    "print(\"obs:\", lovely(state))\n",
    "print(\"metadata:\", env_pong.metadata)\n",
    "print(\"action_space:\", env_pong.action_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(abc.ABC):\n",
    "    def select_action(self, state, collecting_data: bool) -> int:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_scores_parallel(\n",
    "    agent: Agent,\n",
    "    make_env: Callable,\n",
    "    make_env_kwargs: list[dict[str, Any]],\n",
    "    select_action_kwargs: dict[str, Any],\n",
    "    num_episodes: int | None = None,\n",
    "    limit_steps: int = None,\n",
    "    return_unfinished: bool = False,\n",
    "    video_folder: pathlib.Path | str | None = None,\n",
    ") -> Iterable[float]:\n",
    "    if video_folder is not None:\n",
    "        video_folder = pathlib.Path(video_folder)\n",
    "        video_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if isinstance(make_env_kwargs, dict):\n",
    "        make_env_kwargs = [make_env_kwargs] * num_episodes\n",
    "    elif isinstance(make_env_kwargs, list):\n",
    "        if num_episodes is None:\n",
    "            num_episodes = len(make_env_kwargs)\n",
    "    if len(make_env_kwargs) != num_episodes:\n",
    "        raise ValueError(\"num_episodes does not match length of make_env_kwargs\")\n",
    "\n",
    "    def create_env(i, kwargs):\n",
    "        env = make_env(**kwargs)\n",
    "        if video_folder is not None:\n",
    "            env = gym.wrappers.RecordVideo(env, video_folder/str(i))\n",
    "        return env\n",
    "\n",
    "    env = gym.vector.AsyncVectorEnv([\n",
    "        # lambda: create_env(i, kwargs) would not work because of late binding\n",
    "        functools.partial(create_env, i, kwargs) \n",
    "        for i, kwargs in enumerate(make_env_kwargs)\n",
    "    ])\n",
    "    \n",
    "    if limit_steps is None:\n",
    "        limit_steps = env.get_attr(\"spec\")[0].max_episode_steps\n",
    "    if limit_steps is None:\n",
    "        warnings.warn(\"No limit_steps provided and env does not have max_episode_steps set. This might lead to infinite loops.\")\n",
    "    \n",
    "    state = env.reset()\n",
    "    score = np.zeros(num_episodes)\n",
    "    done_in_past = np.zeros(num_episodes, dtype=bool)\n",
    "    done_at_the_moment = np.zeros(num_episodes, dtype=bool)\n",
    "\n",
    "    if limit_steps is None:\n",
    "        steps = itertools.count()\n",
    "    else:\n",
    "        steps = range(limit_steps)\n",
    "    for step in tqdm(steps, \"Measuring score - step\"):\n",
    "        score[done_at_the_moment] = 0\n",
    "        action = agent.select_action(state, **select_action_kwargs)\n",
    "        state, reward, done_at_the_moment, _ = env.step(action)\n",
    "        done_at_the_moment = np.array(done_at_the_moment, dtype=bool)\n",
    "        reward = np.array(reward)\n",
    "        # vector env resets done envs automatically and starts over\n",
    "        # we only want to update score of episodes that were not done before\n",
    "        # and output the score for that env only once\n",
    "        score += reward\n",
    "        done = done_in_past | done_at_the_moment\n",
    "        done_for_first_time = done_in_past != done # this is not the same as done_at_the_moment, because of autoreset\n",
    "        done_in_past = done\n",
    "        \n",
    "        for idx in done_for_first_time.nonzero()[0]:\n",
    "            yield score[idx].item()\n",
    "\n",
    "        if done_in_past.all():\n",
    "            break\n",
    "      \n",
    "    env.close()\n",
    "\n",
    "    if return_unfinished:\n",
    "        yield from score[~done_in_past].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvar(scores: np.ndarray | pd.Series, *, percent: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes conditional value at risk (CVaR) of a given set of scores.\n",
    "    CVaR is the expected value of the worst percents of the scores.\n",
    "    \"\"\"\n",
    "    assert 0 < percent < 100\n",
    "    if not isinstance(scores, pd.Series):\n",
    "        scores = pd.Series(scores)\n",
    "    quantile = scores.quantile(percent / 100)\n",
    "    return scores[scores <= quantile].mean()\n",
    "    \n",
    "cvar(np.arange(100), percent=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(lightning.LightningModule, Agent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config: dict[str, Any],\n",
    "        use_vision: bool,\n",
    "        num_actions: int,\n",
    "        use_double_dqn: bool,\n",
    "        optimizer_class: str,\n",
    "        epsilon: tuple[float, float, int] | float,\n",
    "        use_pretrained_weights: str | None = None,\n",
    "        optimizer_config: dict[str, Any] = None,\n",
    "        make_env: Callable | None = None,\n",
    "        tau: float = 1e-3,\n",
    "        gamma: float = 0.99,\n",
    "        video_root_folder: pathlib.Path | str | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"use_pretrained_weights\", \"make_env\", \"video_root_folder\"])\n",
    "        self.model_config = model_config\n",
    "\n",
    "        if optimizer_config is None:\n",
    "            optimizer_config = {}\n",
    "        self.optimizer_config = optimizer_config\n",
    "        self.optimizer_class = optimizer_class\n",
    "        \n",
    "        # TODO does it matter that they are initialized with the same weights?\n",
    "        if use_vision:\n",
    "            self.qnn = VisionQNet(**model_config, pretrained_weights=use_pretrained_weights)\n",
    "            self.qnn_target = VisionQNet(**model_config, pretrained_weights=use_pretrained_weights)\n",
    "        else:\n",
    "            if use_pretrained_weights is not None:\n",
    "                raise ValueError(\"Pretrained weights only supported for vision models\")\n",
    "            self.qnn = MlpQNet(**model_config)\n",
    "            self.qnn_target = MlpQNet(**model_config)\n",
    "        \n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.use_double_dqn = use_double_dqn\n",
    "        self.epsilon = epsilon\n",
    "        self.iter_collecting_data = 0\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.make_env = make_env\n",
    "        self.validation_step_scores = []\n",
    "        if video_root_folder is None:\n",
    "            self.video_root_folder = None\n",
    "        else:\n",
    "            self.video_root_folder = pathlib.Path(video_root_folder)\n",
    "            self.video_root_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.qnn(observations)\n",
    "\n",
    "    def get_current_epsilon(self) -> float:\n",
    "        if isinstance(self.epsilon, float):\n",
    "            return self.epsilon\n",
    "        upper, lower, decay_steps = self.epsilon\n",
    "        epsilon = upper - self.iter_collecting_data / decay_steps * (upper - lower)\n",
    "        epsilon = max(epsilon, lower)\n",
    "        return epsilon\n",
    "\n",
    "\n",
    "    def select_action(self, observations: torch.Tensor | np.ndarray, collecting_data: bool = False, epsilon: float | None = None) -> np.ndarray:\n",
    "        if epsilon is None:\n",
    "            epsilon = self.get_current_epsilon()\n",
    "            \n",
    "        if collecting_data:\n",
    "            self.iter_collecting_data += 1\n",
    "\n",
    "        if isinstance(observations, np.ndarray):\n",
    "            param = next(self.qnn.parameters())\n",
    "            observations = torch.from_numpy(observations).to(param)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_size = observations.shape[0]\n",
    "            if epsilon or random.random() > self.epsilon:\n",
    "                q_values = self.qnn(observations)\n",
    "                return torch.argmax(q_values, dim=-1).cpu().numpy()\n",
    "            return np.random.randint(self.num_actions, size=batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer_class = getattr(torch.optim, self.optimizer_class)\n",
    "        return optimizer_class(self.qnn.parameters(), **self.optimizer_config)\n",
    "\n",
    "    def training_step(self, batch: Transition, batch_idx):\n",
    "        bs = batch.reward.shape[0]\n",
    "        next_q_value = self.qnn_target(batch.next_state)\n",
    "\n",
    "        if self.use_double_dqn:\n",
    "            next_q_value_local = self.qnn(batch.next_state)\n",
    "            next_action = torch.argmax(next_q_value_local, dim=-1)\n",
    "            next_value_estimate = next_q_value[range(bs), next_action]\n",
    "        else:\n",
    "            next_value_estimate = torch.max(next_q_value, dim=-1).values\n",
    "        \n",
    "        td_target = batch.reward + self.gamma * next_value_estimate * (~batch.done)\n",
    "        td_target = td_target.detach()\n",
    "        td_pred = self.qnn(batch.state)[range(bs), batch.action]\n",
    "        td_error = self.loss_fn(td_pred, td_target)\n",
    "        self.log(\"train/td_error\", td_error)\n",
    "        self.log(\"epsilon\", self.get_current_epsilon())\n",
    "        return td_error\n",
    "    \n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx) -> None:\n",
    "        self.soft_update_target_nn()\n",
    "        return super().on_train_batch_end(outputs, batch, batch_idx)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def soft_update_target_nn(self):\n",
    "        params = zip(self.qnn_target.parameters(), self.qnn.parameters())\n",
    "        for target_param, param in params:\n",
    "            target_param.copy_(self.tau * param + (1.0 - self.tau) * target_param)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if self.video_root_folder is None:\n",
    "            video_folder = None\n",
    "        else:\n",
    "            video_folder = self.video_root_folder / f\"step-{self.trainer.global_step}-batch-{batch_idx}\"\n",
    "        \n",
    "        scores = measure_scores_parallel(\n",
    "            agent=self,\n",
    "            make_env=self.make_env,\n",
    "            make_env_kwargs=batch,\n",
    "            select_action_kwargs={},\n",
    "            video_folder=video_folder,\n",
    "        )\n",
    "        scores = list(scores)\n",
    "        self.validation_step_scores.extend(scores)\n",
    "\n",
    "        if batch_idx == 0 and video_folder is not None:\n",
    "            for logger in self.loggers:\n",
    "                if isinstance(logger, lightning.pytorch.loggers.WandbLogger):\n",
    "                    for worker_dir in video_folder.glob(\"*\"):\n",
    "                        logger.log_metrics({\n",
    "                            f\"video/worker_{worker_dir.name}\": wandb.Video(str(video))\n",
    "                            for video in worker_dir.glob(\"*0.mp4\")\n",
    "                        }, step=self.trainer.global_step)\n",
    "                    break\n",
    "\n",
    "        return scores\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        scores = torch.tensor(self.validation_step_scores)\n",
    "        self.log_simulation_metrics(scores)\n",
    "        self.validation_step_scores.clear()\n",
    "\n",
    "    def simulation_metrics(self, scores: torch.Tensor | np.ndarray) -> dict[str, float]:\n",
    "        metrics = {\n",
    "            \"score/avg\": float(scores.mean()),\n",
    "            \"score/std\": float(scores.std()),\n",
    "            \"score/min\": float(scores.min()),\n",
    "            \"score/max\": float(scores.max()),\n",
    "            \"score/num_episodes\": float(len(scores)),\n",
    "        }\n",
    "        for p in [25, 50, 75]:\n",
    "            metrics[f\"score/percentile_{p}\"] = float(np.percentile(scores, p))\n",
    "            metrics[f\"score/cvar_{p}\"] = float(cvar(scores, percent=p))\n",
    "        return metrics\n",
    "\n",
    "    def log_simulation_metrics(self, scores: Iterable[float], metrics: dict[str, float] | None = None):\n",
    "        if metrics is None:\n",
    "            metrics = self.simulation_metrics(scores)\n",
    "\n",
    "        for logger in self.loggers:\n",
    "            if isinstance(logger, lightning.pytorch.loggers.WandbLogger):\n",
    "                logger.log_metrics({f\"score/score\": wandb.Histogram(scores)}, step=self.global_step)\n",
    "        \n",
    "        self.log_dict(metrics)\n",
    "            \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorParallel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent: Agent,\n",
    "        make_env: Callable[[int | None, bool], gym.Env],\n",
    "        make_env_kwargs: dict[str, Any] | list[dict[str, Any]] | None = None,\n",
    "        num_parallel_envs: int | None = None,\n",
    "    ) -> None:\n",
    "        if num_parallel_envs is None:\n",
    "            if make_env_kwargs is None:\n",
    "                raise ValueError(\"Either make_env_kwargs or num_parallel_envs must be set\")\n",
    "            if isinstance(make_env_kwargs, dict):\n",
    "                raise ValueError(\"num_parallel_envs must be set if make_env_kwargs is a dict\")\n",
    "        \n",
    "        if make_env_kwargs is None:\n",
    "            make_env_kwargs = {}\n",
    "        if isinstance(make_env_kwargs, dict):\n",
    "            make_env_kwargs = [make_env_kwargs for _ in range(num_parallel_envs)]\n",
    "        if num_parallel_envs is None:\n",
    "            num_parallel_envs = len(make_env_kwargs)\n",
    "        if len(make_env_kwargs) != num_parallel_envs:\n",
    "            raise ValueError(\"make_env_kwargs must have the same length as num_parallel_envs\")\n",
    "\n",
    "        self.agent = agent\n",
    "        self.make_env = make_env\n",
    "        self.make_env_kwargs = make_env_kwargs\n",
    "        self.num_parallel_envs = num_parallel_envs\n",
    "\n",
    "    def __iter__(self) -> Iterable[Transition]:\n",
    "        envs = gym.vector.AsyncVectorEnv([\n",
    "            functools.partial(self.make_env, **kwargs)\n",
    "            for kwargs in self.make_env_kwargs\n",
    "        ])\n",
    "        states = envs.reset()\n",
    "        while True:\n",
    "            actions = self.agent.select_action(states, collecting_data=True)\n",
    "            next_states, rewards, dones, _ = envs.step(actions)\n",
    "            for s, a, r, s_, d in zip(states, actions, rewards, next_states, dones):\n",
    "                yield Transition(s, a, r, s_, d)\n",
    "            states = next_states\n",
    "\n",
    "\n",
    "class DataCollectionCallback(lightning.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        parallel_experience_generator: DataGeneratorParallel,\n",
    "        buffer: ReplayBuffer,\n",
    "        collect_every_n_steps: int,\n",
    "        collect_num_steps_in_each_env: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.parallel_experience_collector = parallel_experience_generator\n",
    "        self.buffer = buffer\n",
    "        self.collect_every_n_steps = collect_every_n_steps\n",
    "        self.collect_num_steps_in_each_env = collect_num_steps_in_each_env\n",
    "        self.experience_iterator = iter(self.parallel_experience_collector)\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module) -> None:\n",
    "        self.collect_experience()\n",
    "\n",
    "    def on_train_batch_end(self, trainer: lightning.Trainer, pl_module, outputs, batch, batch_idx) -> None:\n",
    "        if trainer.global_step % self.collect_every_n_steps == 0:\n",
    "            self.collect_experience()\n",
    "\n",
    "    def collect_experience(self):\n",
    "        for step in range(self.collect_num_steps_in_each_env):\n",
    "            transition = next(self.experience_iterator)\n",
    "            self.buffer.add(transition)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epsilon_upper = 0.60\n",
    "epsilon_lower = 0.01\n",
    "epsilon_decay_steps = 10_000\n",
    "gamma = 0.99\n",
    "tau = 0.01\n",
    "batch_size = 128\n",
    "use_double_dqn = True\n",
    "\n",
    "buffer_capacity = 100_000\n",
    "collect_data_num_parallel_envs = 10\n",
    "collect_data_every_n_steps = 1000 # TODO\n",
    "collect_data_num_steps_in_each_env = 2500  # TODO\n",
    "\n",
    "val_check_interval = 1000\n",
    "valid_num_parallel_envs = 10\n",
    "valid_num_batches = 1\n",
    "\n",
    "use_pretrained_weights = None # set 'DEFAULT' to use weights from torchvision\n",
    "\n",
    "accelerator = \"cpu\"\n",
    "precision = \"16-mixed\"\n",
    "devices = []\n",
    "\n",
    "training_env = \"pong\"\n",
    "model_config_name = \"efficientnet-b0\" # or \"mobilenet_v3_small\" or \"mlp\"\n",
    "\n",
    "checkpoint_monitor = (\"score/avg\", \"max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_devices = \"auto\" if accelerator == \"cpu\" else devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_env not in [\"pong\", \"gridworld\"]:\n",
    "    raise ValueError(\"training_env must be either 'pong' or 'gridworld'\")\n",
    "\n",
    "if training_env == \"pong\":\n",
    "    make_env = make_env_pong\n",
    "else:\n",
    "    make_env = make_env_minigrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = make_env().action_space.n\n",
    "\n",
    "mlp_config = {\n",
    "    \"num_actions\": num_actions,\n",
    "}\n",
    "\n",
    "mobilenet_v3_small_config = {\n",
    "    \"actions\": num_actions,\n",
    "    \"architecture\": \"mobilenet_v3_small\",\n",
    "    \"input_conv\": \"features.0.0\",\n",
    "    \"output_lin\": \"classifier.3\",\n",
    "    \"input_channels\": 4,\n",
    "}\n",
    "\n",
    "efficientnet_b0_config = {\n",
    "    \"actions\": num_actions,\n",
    "    \"architecture\": \"efficientnet_b0\",\n",
    "    \"input_conv\": \"features.0.0\",\n",
    "    \"output_lin\": \"classifier.1\",\n",
    "    \"input_channels\": 4,\n",
    "}\n",
    "\n",
    "if model_config_name == \"mlp\":\n",
    "    model_config = mlp_config\n",
    "elif model_config_name == \"mobilenet_v3_small\":\n",
    "    model_config = mobilenet_v3_small_config\n",
    "elif model_config_name == \"efficientnet-b0\":\n",
    "    model_config = efficientnet_b0_config\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model_config: {model_config_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./wandb\", exist_ok=True)\n",
    "os.makedirs(\"./videos\", exist_ok=True)\n",
    "\n",
    "wandb_logger = lightning.pytorch.loggers.WandbLogger(\n",
    "    project=\"jku-deep-rl_dqn\",\n",
    "    group=f\"dqn-{training_env}\",\n",
    "    save_dir=\"./wandb\",\n",
    ")\n",
    "wandb_experiment: wandb.wandb_run.Run = wandb_logger.experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(\n",
    "    make_env = make_env,\n",
    "    model_config = model_config,\n",
    "    use_vision = model_config != mlp_config,\n",
    "    num_actions = num_actions,\n",
    "    use_double_dqn = use_double_dqn,\n",
    "    optimizer_class = \"AdamW\",\n",
    "    optimizer_config = dict(lr=learning_rate),\n",
    "    epsilon = (epsilon_upper, epsilon_lower, epsilon_decay_steps),\n",
    "    gamma = gamma,\n",
    "    tau = tau,\n",
    "    use_pretrained_weights = use_pretrained_weights,\n",
    "    video_root_folder = f\"./videos/{wandb_experiment.name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(capacity=buffer_capacity, seed=42)\n",
    "\n",
    "experience_generator = DataGeneratorParallel(\n",
    "    agent = dqn,\n",
    "    make_env = make_env,\n",
    "    make_env_kwargs = [{} for _ in range(collect_data_num_parallel_envs)],\n",
    "    num_parallel_envs = collect_data_num_parallel_envs,\n",
    ")\n",
    "\n",
    "experience_collection_callback = DataCollectionCallback(\n",
    "    parallel_experience_generator = experience_generator,\n",
    "    buffer = buffer,\n",
    "    collect_every_n_steps = collect_data_every_n_steps,\n",
    "    collect_num_steps_in_each_env = collect_data_num_steps_in_each_env,\n",
    ")\n",
    "\n",
    "monitor_metric, metric_mode = checkpoint_monitor\n",
    "checkpointing = lightning.pytorch.callbacks.ModelCheckpoint(\n",
    "    monitor=monitor_metric,\n",
    "    mode=metric_mode,\n",
    "    save_last=True,\n",
    "    save_top_k=10,\n",
    "    dirpath=f\"checkpoints/{wandb_logger.experiment.name}/\",\n",
    "    filename=\"step={step}_score_avg={score/avg:.2f}\",\n",
    "    auto_insert_metric_name=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    buffer,\n",
    "    batch_size = batch_size,\n",
    "    pin_memory = True,\n",
    "    shuffle = False, # ReplayBuffer already samples randomly\n",
    ")\n",
    "\n",
    "valid_make_env_kwargs = [\n",
    "    {\"seed\": env_idx * valid_num_batches + batch_ids}\n",
    "    for env_idx in range(valid_num_parallel_envs) for batch_ids in range(valid_num_batches)\n",
    "]\n",
    "\n",
    "# validation loader just gives parameters for creating the gym env\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_make_env_kwargs,\n",
    "    batch_size = valid_num_parallel_envs,\n",
    "    shuffle = False,\n",
    "    drop_last = False,\n",
    "    collate_fn=lambda x: x,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger.experiment.config.update({\n",
    "    \"use_pretrained_weights\": use_pretrained_weights,\n",
    "    \"buffer_capacity\": buffer_capacity,\n",
    "    \"collect_data\": {\n",
    "        \"num_parallel_envs\": collect_data_num_parallel_envs,\n",
    "        \"every_n_steps\": collect_data_every_n_steps,\n",
    "        \"num_steps_in_each_env\": collect_data_num_steps_in_each_env,\n",
    "    },\n",
    "    \"checkpointing\": {\n",
    "        \"monitor\": checkpointing.monitor,\n",
    "        \"mode\": checkpointing.mode,\n",
    "        \"save_last\": checkpointing.save_last,\n",
    "        \"save_top_k\": checkpointing.save_top_k,\n",
    "    },\n",
    "    \"train_loader\": {\n",
    "        \"batch_size\": train_loader.batch_size,\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"num_parallel_envs\": valid_num_parallel_envs,\n",
    "        \"make_env_kwargs\": valid_make_env_kwargs,\n",
    "        \"total_runs\": len(valid_make_env_kwargs),\n",
    "    }\n",
    "})\n",
    "\n",
    "wandb_logger.experiment.tags += (\n",
    "    \"double-dqn-on\" if dqn.use_double_dqn else \"double-dqn-off\",\n",
    "    f\"init-pretrained-{use_pretrained_weights}\",\n",
    ")\n",
    "\n",
    "if \"architecture\" in dqn.model_config:\n",
    "    wandb_logger.experiment.tags += tuple([\n",
    "        f\"architecture-{dqn.model_config['architecture']}\",\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = lightning.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=-1,\n",
    "    val_check_interval=val_check_interval,\n",
    "    devices=trainer_devices,\n",
    "    precision=precision,\n",
    "    accelerator=accelerator,\n",
    "    callbacks=[\n",
    "        experience_collection_callback,\n",
    "        checkpointing,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    dqn,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=valid_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl-imitation-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
