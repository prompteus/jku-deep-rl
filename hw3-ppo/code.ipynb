{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import abc\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "import functools\n",
    "import random\n",
    "import warnings\n",
    "import pathlib\n",
    "from typing import Iterable, Callable, Any, NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import lightning\n",
    "import lightning.pytorch\n",
    "import lightning.pytorch.loggers\n",
    "import lightning.pytorch.callbacks\n",
    "import gymnasium as gym\n",
    "import lovely_tensors\n",
    "import lovely_numpy\n",
    "import wandb\n",
    "import wandb.wandb_run\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def lovely(x):\n",
    "    \"summarizes important tensor properties\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return lovely_numpy.lovely(x)\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        return lovely_tensors.lovely(x)\n",
    "    else:\n",
    "        warnings.warn(f\"lovely: unknown type {type(x)}\")\n",
    "        return str(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied and adapted from previous assignment\n",
    "class Transition(NamedTuple):\n",
    "    state: torch.Tensor | np.ndarray\n",
    "    action: torch.Tensor | np.ndarray\n",
    "    reward: torch.Tensor | np.ndarray\n",
    "    next_state: torch.Tensor | np.ndarray\n",
    "    action_log_prob: torch.Tensor | np.ndarray\n",
    "    done: torch.Tensor | np.ndarray | bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied and adapted from previous assignment\n",
    "class ReplayBuffer(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, capacity: int, seed = None):\n",
    "        self.capacity = capacity\n",
    "        self.buffer: list[Transition] = []\n",
    "        self.seed = seed\n",
    "        self.rng = random.Random(seed)\n",
    "    \n",
    "    def __next__(self):\n",
    "        transition = self.rng.choice(self.buffer)\n",
    "        return Transition(\n",
    "            state = torch.tensor(transition.state, dtype=torch.float32),\n",
    "            action = torch.tensor(transition.action, dtype=torch.int64),\n",
    "            reward = torch.tensor(transition.reward, dtype=torch.float32),\n",
    "            next_state = torch.tensor(transition.next_state, dtype=torch.float32),\n",
    "            action_log_prob = torch.tensor(transition.action_log_prob, dtype=torch.float32),\n",
    "            done = torch.tensor(transition.done.item(), dtype=torch.bool),\n",
    "        )\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def add(self, t: Transition):\n",
    "        self.buffer.append(t)\n",
    "        if len(self.buffer) > self.capacity:\n",
    "            self.buffer.pop(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetArgsWrapper(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, reset_kwargs: dict[str, Any]):\n",
    "        super().__init__(env)\n",
    "        self.reset_kwargs = reset_kwargs\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        kwargs = {**self.reset_kwargs, **kwargs}\n",
    "        return self.env.reset(*args, **kwargs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ResetArgsWrapper({self.env}, {self.reset_kwargs})\"\n",
    "\n",
    "\n",
    "def make_bipedal_walker_env(hardcore=False, seed: int | None = None, render_mode: str = \"rgb_array\", **kwargs):\n",
    "    env: gym.Environment = gym.make(\n",
    "        'BipedalWalker-v3',\n",
    "        render_mode=render_mode,\n",
    "        hardcore=hardcore,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    if seed is not None:\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        env = ResetArgsWrapper(env, {\"seed\": seed})\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [[0, 0, 0], [1, 2, 3]]\n",
    "\n",
    "for hardcore in [False, True]:\n",
    "    fig = plt.figure()\n",
    "    for row, row_seeds in enumerate(seeds):\n",
    "        for col, seed in enumerate(row_seeds):\n",
    "            env = make_bipedal_walker_env(seed=seed, hardcore=hardcore)\n",
    "            env.reset()\n",
    "            img = env.render()\n",
    "            env.close()\n",
    "            fig.add_subplot(len(seeds), len(row_seeds), row * len(row_seeds) + col + 1)\n",
    "            plt.imshow(img)\n",
    "            # set img title\n",
    "            plt.title(f\"seed = {seed}\")\n",
    "            plt.axis('off')\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(f\"hardcore = {hardcore}\")\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        around: torch.nn.Module\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.around = around\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y: torch.Tensor = self.around(x)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        norm_layer: torch.nn.Module,\n",
    "        hidden_activation: torch.nn.Module | None,\n",
    "        output_activation: torch.nn.Module | None,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        is_residual: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.is_residual = is_residual\n",
    "\n",
    "        dims = [input_dim] + [hidden_dim] * (num_layers - 1) + [output_dim]\n",
    "        layers = []\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            layers.append(torch.nn.Linear(dims[i], dims[i + 1]))\n",
    "\n",
    "            if i < num_layers - 1:\n",
    "                if hidden_activation is not None:\n",
    "                    layers.append(copy.deepcopy(hidden_activation))\n",
    "\n",
    "            else:\n",
    "                if norm_layer is not None:\n",
    "                    layers.append(copy.deepcopy(norm_layer))\n",
    "                if output_activation is not None:\n",
    "                    layers.append(copy.deepcopy(output_activation))\n",
    "                if dropout > 0:\n",
    "                    layers.append(torch.nn.Dropout(dropout))\n",
    "\n",
    "        self.nn = torch.nn.Sequential(*layers)\n",
    "\n",
    "        if self.is_residual:\n",
    "            if self.input_dim != self.output_dim:\n",
    "                raise ValueError(\"Residual block input and output dimensions must match\")\n",
    "            self.nn = Residual(self.nn)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.nn(x)\n",
    "\n",
    "\n",
    "class ResFeedForward(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_blocks: int,\n",
    "        activation: torch.nn.Module,\n",
    "        output_dim: int | None = None,\n",
    "        squeeze_dim: int | None = None,\n",
    "        use_skip: bool = True,\n",
    "    ) -> None:\n",
    "        if squeeze_dim is None:\n",
    "            squeeze_dim = hidden_dim\n",
    "        if output_dim is None:\n",
    "            output_dim = hidden_dim\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.squeeze_dim = squeeze_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        dims = [input_dim] + [hidden_dim] * num_blocks + [output_dim]\n",
    "        blocks = []\n",
    "\n",
    "        for i, (block_in_dim, block_out_dim) in enumerate(zip(dims[:-1], dims[1:])):\n",
    "            block_activation = activation if i < num_blocks - 1 else None\n",
    "            block = Block(\n",
    "                input_dim=block_in_dim,\n",
    "                hidden_dim=squeeze_dim,\n",
    "                output_dim=block_out_dim,\n",
    "                norm_layer=torch.nn.BatchNorm1d(block_out_dim),\n",
    "                hidden_activation=activation,\n",
    "                output_activation=block_activation,\n",
    "                is_residual=use_skip and block_in_dim == block_out_dim,\n",
    "            )\n",
    "            blocks.append(block)\n",
    "            \n",
    "        self.nn = torch.nn.Sequential(*blocks)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.nn(x)\n",
    "    \n",
    "\n",
    "class ContinuousActorNet(torch.nn.Module):\n",
    "    def __init__(self, backbone: torch.nn.Module, output_dim: int, epsilon: float = 0.001) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head_mu = torch.nn.LazyLinear(output_dim)\n",
    "        self.head_sigma = torch.nn.LazyLinear(output_dim)\n",
    "        \n",
    "    def forward(self, state: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        state = self.backbone(state)\n",
    "        mu_logits = self.head_mu(state)\n",
    "        sigma_logits = self.head_sigma(state)\n",
    "        return mu_logits, sigma_logits\n",
    "    \n",
    "    def distribution(self, mu_logits: torch.Tensor, sigma_logits: torch.Tensor) -> torch.distributions.Distribution:\n",
    "        mu_logits = torch.nn.functional.tanh(mu_logits)\n",
    "        sigma_logits = torch.nn.functional.softplus(sigma_logits) + 1e-5\n",
    "        distribution = torch.distributions.MultivariateNormal(mu_logits, torch.diag_embed(sigma_logits))\n",
    "        return distribution\n",
    "\n",
    "    def act(self, state: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        mu_logits, sigma_logits = self.forward(state)\n",
    "        return self.sample(mu_logits, sigma_logits)\n",
    "\n",
    "    def sample(self, mu_logits: torch.Tensor, sigma_logits: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        distribution = self.distribution(mu_logits, sigma_logits)\n",
    "        action = distribution.rsample()\n",
    "        log_prob = distribution.log_prob(action)\n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "class CriticNet(torch.nn.Module):\n",
    "    def __init__(self, backbone: torch.nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head_state_value = torch.nn.LazyLinear(1)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        state = self.backbone(state)\n",
    "        state_value = self.head_state_value(state)\n",
    "        return state_value\n",
    "\n",
    "    \n",
    "class Agent(abc.ABC):\n",
    "    def select_action(self, observations: torch.Tensor, collecting_data: bool) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns actions and log_probs for a batch of observations\n",
    "        \"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied and adapted from previous assignment\n",
    "def measure_scores_parallel(\n",
    "    agent: Agent,\n",
    "    make_env: Callable,\n",
    "    make_env_kwargs: list[dict[str, Any]],\n",
    "    num_episodes: int | None = None,\n",
    "    limit_steps: int = None,\n",
    "    return_unfinished: bool = False,\n",
    "    video_folder: pathlib.Path | str | None = None,\n",
    "    select_action_kwargs: dict[str, Any] = None,\n",
    ") -> Iterable[float]:\n",
    "    if video_folder is not None:\n",
    "        video_folder = pathlib.Path(video_folder)\n",
    "        video_folder.mkdir(parents=True, exist_ok=True)\n",
    "    if select_action_kwargs is None:\n",
    "        select_action_kwargs = {}\n",
    "    if isinstance(make_env_kwargs, dict):\n",
    "        make_env_kwargs = [make_env_kwargs] * num_episodes\n",
    "    elif isinstance(make_env_kwargs, list):\n",
    "        if num_episodes is None:\n",
    "            num_episodes = len(make_env_kwargs)\n",
    "    if len(make_env_kwargs) != num_episodes:\n",
    "        raise ValueError(\"num_episodes does not match length of make_env_kwargs\")\n",
    "\n",
    "    def create_env(i, kwargs):\n",
    "        env = make_env(**kwargs)\n",
    "        if video_folder is not None:\n",
    "            env = gym.wrappers.RecordVideo(env, video_folder/str(i), disable_logger=True)\n",
    "        return env\n",
    "\n",
    "    env = gym.vector.AsyncVectorEnv([\n",
    "        # lambda: create_env(i, kwargs) would not work because of late binding\n",
    "        functools.partial(create_env, i, kwargs) \n",
    "        for i, kwargs in enumerate(make_env_kwargs)\n",
    "    ])\n",
    "    \n",
    "    if limit_steps is None:\n",
    "        limit_steps = env.get_attr(\"spec\")[0].max_episode_steps\n",
    "    if limit_steps is None:\n",
    "        warnings.warn(\"No limit_steps provided and env does not have max_episode_steps set. This might lead to infinite loops.\")\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    score = np.zeros(num_episodes)\n",
    "    done_in_past = np.zeros(num_episodes, dtype=bool)\n",
    "    done_at_the_moment = np.zeros(num_episodes, dtype=bool)\n",
    "\n",
    "    if limit_steps is None:\n",
    "        steps = itertools.count()\n",
    "    else:\n",
    "        steps = range(limit_steps)\n",
    "    for step in tqdm(steps, \"Measuring score - step\"):\n",
    "        score[done_at_the_moment] = 0\n",
    "        action, log_prob = agent.select_action(state, **select_action_kwargs)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done_at_the_moment = terminated | truncated\n",
    "        # vector env resets done envs automatically and starts over\n",
    "        # we only want to update score of episodes that were not done before\n",
    "        # and output the score for that env only once\n",
    "        score += reward\n",
    "        done = done_in_past | done_at_the_moment\n",
    "        done_for_first_time = done_in_past != done # this is not the same as done_at_the_moment, because of autoreset\n",
    "        done_in_past = done\n",
    "        \n",
    "        for idx in done_for_first_time.nonzero()[0]:\n",
    "            yield score[idx].item()\n",
    "\n",
    "        if done_in_past.all():\n",
    "            break\n",
    "      \n",
    "    env.close()\n",
    "\n",
    "    if return_unfinished:\n",
    "        yield from score[~done_in_past].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from previous assignment\n",
    "def cvar(scores: np.ndarray | pd.Series, *, percent: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes conditional value at risk (CVaR) of a given set of scores.\n",
    "    CVaR is the expected value of the worst percents of the scores.\n",
    "    \"\"\"\n",
    "    assert 0 < percent < 100\n",
    "    if not isinstance(scores, pd.Series):\n",
    "        scores = pd.Series(scores)\n",
    "    quantile = scores.quantile(percent / 100)\n",
    "    return scores[scores <= quantile].mean()\n",
    "    \n",
    "cvar(np.arange(100), percent=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "\n",
    "class PPOCriticLossInfo(NamedTuple):\n",
    "    td_loss: torch.Tensor\n",
    "    td_advantage: torch.Tensor\n",
    "\n",
    "class PPOActorLossInfo(NamedTuple):\n",
    "    ppo_loss: torch.Tensor\n",
    "    entropy: torch.Tensor\n",
    "    was_clipped: torch.Tensor\n",
    "    importance_ratio: torch.Tensor\n",
    "    \n",
    "\n",
    "class PPO(lightning.LightningModule, Agent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_config: dict[str, Any],\n",
    "        output_dim: int,\n",
    "        optimizer_class: str,\n",
    "        optimizer_config: dict[str, Any],\n",
    "        make_env: Callable | None = None,\n",
    "        video_root_folder: pathlib.Path | str | None = None,\n",
    "        importance_sampling_clip_range: tuple[float, float] = (0.8, 1.2),\n",
    "        discount: float = 0.99,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"make_env\", \"video_root_folder\"])\n",
    "        self.actor = ContinuousActorNet(ResFeedForward(**backbone_config), output_dim)\n",
    "        self.critic = CriticNet(ResFeedForward(**backbone_config))\n",
    "        self.critic_loss = torch.nn.MSELoss()\n",
    "        self.make_env = make_env\n",
    "        self.validation_step_scores = []\n",
    "        self.validation_start_time = None\n",
    "        if video_root_folder is None:\n",
    "            self.video_root_folder = None\n",
    "        else:\n",
    "            self.video_root_folder = pathlib.Path(video_root_folder)\n",
    "            self.video_root_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer_class = getattr(torch.optim, self.hparams.optimizer_class)\n",
    "        optimizer = optimizer_class(self.parameters(), **self.hparams.optimizer_config)\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, observations: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.actor(observations)\n",
    "    \n",
    "    def select_action(self, observation: torch.Tensor | np.ndarray, collecting_data: bool = False) -> tuple[np.ndarray, np.ndarray]:\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            param = next(self.actor.parameters())\n",
    "            observation = torch.from_numpy(observation).to(param)\n",
    "        action, log_prob = self.actor.act(observation)\n",
    "        action: torch.Tensor = action.detach().cpu().float().numpy()\n",
    "        log_prob: torch.Tensor = log_prob.detach().cpu().float().numpy()\n",
    "        return action, log_prob\n",
    "\n",
    "    def _critic_loss(self, batch: Transition) -> PPOCriticLossInfo:\n",
    "        next_state_value_estimate = self.critic(batch.next_state).detach().flatten()\n",
    "        curr_state_value_estimate = self.critic(batch.state).flatten()\n",
    "        td_target = batch.reward + self.hparams.discount * next_state_value_estimate * (~batch.done)\n",
    "        td_loss = self.critic_loss(curr_state_value_estimate, td_target)\n",
    "        td_advantage = (td_target - curr_state_value_estimate).detach()\n",
    "        return PPOCriticLossInfo(td_loss, td_advantage)\n",
    "\n",
    "    def _actor_loss(self, batch: Transition, td_advantage: torch.Tensor) -> PPOActorLossInfo:\n",
    "        mu_logits, sigma_logits = self.actor(batch.state)\n",
    "        distribution = self.actor.distribution(mu_logits, sigma_logits)\n",
    "        action_log_prob_new = distribution.log_prob(batch.action)\n",
    "        action_log_prob_old = batch.action_log_prob\n",
    "        clip_min, clip_max = self.hparams.importance_sampling_clip_range\n",
    "        importance_ratio = torch.exp(action_log_prob_new - action_log_prob_old)\n",
    "        importance_ratio_clipped = torch.clip(importance_ratio, clip_min, clip_max)\n",
    "        weighed_td_advantage = td_advantage * importance_ratio\n",
    "        weighed_td_advantage_clipped = td_advantage * importance_ratio_clipped\n",
    "        objective = torch.minimum(weighed_td_advantage, weighed_td_advantage_clipped)\n",
    "        was_clipped = weighed_td_advantage_clipped < weighed_td_advantage \n",
    "        ppo_loss = -objective.mean()\n",
    "        entropy = distribution.entropy().mean()\n",
    "        return PPOActorLossInfo(ppo_loss, entropy, was_clipped, importance_ratio)\n",
    "\n",
    "    def training_step(self, batch: Transition) -> torch.Tensor:        \n",
    "        critic_info = self._critic_loss(batch)\n",
    "        actor_info = self._actor_loss(batch, critic_info.td_advantage)\n",
    "\n",
    "        self.log_dict({\n",
    "            \"critic_td_loss\": critic_info.td_loss,\n",
    "            \"actor_ppo_loss\": actor_info.ppo_loss,\n",
    "            \"td_advantage\": critic_info.td_advantage.mean(),\n",
    "            \"was_clipped\": actor_info.was_clipped.float().mean(),\n",
    "            \"importance_ratio_min\": actor_info.importance_ratio.min(),\n",
    "            \"importance_ratio_max\": actor_info.importance_ratio.max(),\n",
    "        })\n",
    "        \n",
    "        # TODO add entropy regularization to encourage early exploration\n",
    "        return critic_info.td_loss + actor_info.ppo_loss\n",
    "\n",
    "    def validation_step(self, make_env_kwargs: list[dict[str, Any]], batch_idx):\n",
    "        \"\"\"\n",
    "        Run a batch of validation episodes and log the scores.\n",
    "        make_env_kwargs is a list of dictionaries of keyword arguments, each of which is passed to make_env.\n",
    "        \"\"\"\n",
    "        if self.video_root_folder is None:\n",
    "            video_folder = None\n",
    "        else:\n",
    "            video_folder = self.video_root_folder / f\"step-{self.trainer.global_step}-batch-{batch_idx}\"\n",
    "        \n",
    "        scores = measure_scores_parallel(\n",
    "            agent=self,\n",
    "            make_env=self.make_env,\n",
    "            make_env_kwargs=make_env_kwargs,\n",
    "            video_folder=video_folder,\n",
    "        )\n",
    "        scores = list(scores)\n",
    "        self.validation_step_scores.extend(scores)\n",
    "\n",
    "        if video_folder is not None:\n",
    "            for logger in self.loggers:\n",
    "                if isinstance(logger, lightning.pytorch.loggers.WandbLogger):\n",
    "                    for worker_dir in video_folder.glob(\"*\"):\n",
    "                        logger.log_metrics({\n",
    "                            f\"video/batch_{batch_idx}_worker_{worker_dir.name}\": wandb.Video(str(video))\n",
    "                            for video in worker_dir.glob(\"*0.mp4\")\n",
    "                        }, step=self.trainer.global_step)\n",
    "                    break\n",
    "\n",
    "        return scores\n",
    "    \n",
    "    def on_validation_epoch_start(self):\n",
    "        self.validation_step_scores.clear()\n",
    "        self.validation_start_time = time.time()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        scores = torch.tensor(self.validation_step_scores)\n",
    "        validation_num_seconds = time.time() - self.validation_start_time\n",
    "        self.log_simulation_metrics(scores, validation_num_seconds)\n",
    "\n",
    "    def simulation_metrics(self, scores: torch.Tensor | np.ndarray) -> dict[str, float]:\n",
    "        metrics = {\n",
    "            \"score/avg\": float(scores.mean()),\n",
    "            \"score/std\": float(scores.std()),\n",
    "            \"score/min\": float(scores.min()),\n",
    "            \"score/max\": float(scores.max()),\n",
    "            \"score/num_episodes\": float(len(scores)),\n",
    "        }\n",
    "        for p in [25, 50, 75]:\n",
    "            metrics[f\"score/percentile_{p}\"] = float(np.percentile(scores, p))\n",
    "            metrics[f\"score/cvar_{p}\"] = float(cvar(scores, percent=p))\n",
    "        return metrics\n",
    "\n",
    "    def log_simulation_metrics(self, scores: Iterable[float], validation_num_seconds: float, metrics: dict[str, float] | None = None):\n",
    "        if metrics is None:\n",
    "            metrics = self.simulation_metrics(scores)\n",
    "        metrics[\"score/validation_total_num_seconds\"] = validation_num_seconds\n",
    "\n",
    "        for logger in self.loggers:\n",
    "            if isinstance(logger, lightning.pytorch.loggers.WandbLogger):\n",
    "                logger.log_metrics({f\"score/score\": wandb.Histogram(scores)}, step=self.trainer.global_step)\n",
    "\n",
    "        self.log_dict(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied and adjusted from previous assignment\n",
    "class DataGeneratorParallel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent: Agent,\n",
    "        make_env: Callable[[int | None, bool], gym.Env],\n",
    "        make_env_kwargs: dict[str, Any] | list[dict[str, Any]] | None = None,\n",
    "        num_parallel_envs: int | None = None,\n",
    "    ) -> None:\n",
    "        if num_parallel_envs is None:\n",
    "            if make_env_kwargs is None:\n",
    "                raise ValueError(\"Either make_env_kwargs or num_parallel_envs must be set\")\n",
    "            if isinstance(make_env_kwargs, dict):\n",
    "                raise ValueError(\"num_parallel_envs must be set if make_env_kwargs is a dict\")\n",
    "        \n",
    "        if make_env_kwargs is None:\n",
    "            make_env_kwargs = {}\n",
    "        if isinstance(make_env_kwargs, dict):\n",
    "            make_env_kwargs = [make_env_kwargs for _ in range(num_parallel_envs)]\n",
    "        if num_parallel_envs is None:\n",
    "            num_parallel_envs = len(make_env_kwargs)\n",
    "        if len(make_env_kwargs) != num_parallel_envs:\n",
    "            raise ValueError(\"make_env_kwargs must have the same length as num_parallel_envs\")\n",
    "\n",
    "        self.agent = agent\n",
    "        self.make_env = make_env\n",
    "        self.make_env_kwargs = make_env_kwargs\n",
    "        self.num_parallel_envs = num_parallel_envs\n",
    "\n",
    "    def __iter__(self) -> Iterable[Transition]:\n",
    "        envs = gym.vector.AsyncVectorEnv([\n",
    "            functools.partial(self.make_env, **kwargs)\n",
    "            for kwargs in self.make_env_kwargs\n",
    "        ])\n",
    "        state, _ = envs.reset()\n",
    "        while True:\n",
    "            action, log_prob = self.agent.select_action(state, collecting_data=True)\n",
    "            next_state, reward, terminated, truncated, _ = envs.step(action)\n",
    "            done = terminated | truncated\n",
    "            batch = zip(state, action, reward, next_state, log_prob, done)\n",
    "            for s, a, r, s_, l, d in batch:\n",
    "                yield Transition(s, a, r, s_, l, d)\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "class DataCollectionCallback(lightning.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        parallel_experience_generator: DataGeneratorParallel,\n",
    "        buffer: ReplayBuffer,\n",
    "        collect_every_n_updates: int,\n",
    "        collect_num_steps_in_total: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.parallel_experience_collector = parallel_experience_generator\n",
    "        self.buffer = buffer\n",
    "        self.collect_every_n_updates = collect_every_n_updates\n",
    "        self.collect_num_steps_total = collect_num_steps_in_total\n",
    "        if collect_num_steps_in_total % parallel_experience_generator.num_parallel_envs != 0:\n",
    "            raise ValueError(\"collect_num_steps_in_total must be divisible by num_parallel_envs\")\n",
    "        self.collect_num_batch_steps = collect_num_steps_in_total // parallel_experience_generator.num_parallel_envs\n",
    "        self.experience_iterator = iter(self.parallel_experience_collector)\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module: lightning.LightningModule) -> None:\n",
    "        self.collect_experience(pl_module)\n",
    "\n",
    "    def on_train_batch_end(self, trainer: lightning.Trainer, pl_module: lightning.LightningModule, outputs, batch, batch_idx) -> None:\n",
    "        if trainer.global_step % self.collect_every_n_updates == 0:\n",
    "            self.collect_experience(pl_module)\n",
    "\n",
    "    def collect_experience(self, pl_module: lightning.LightningModule):\n",
    "        wall_time = time.time()\n",
    "        for step in range(self.collect_num_batch_steps):\n",
    "            transition = next(self.experience_iterator)\n",
    "            self.buffer.add(transition)\n",
    "        num_seconds = time.time() - wall_time\n",
    "        num_parallel_envs = self.parallel_experience_collector.num_parallel_envs\n",
    "        metrics = {\n",
    "            \"collect_experience/duration_seconds\": num_seconds,\n",
    "            \"collect_experience/num_steps_total\": float(self.collect_num_steps_total),\n",
    "            \"collect_experience/num_steps_in_each_env\": float(self.collect_num_batch_steps),\n",
    "            \"collect_experience/num_batch_steps_per_second\": self.collect_num_batch_steps / num_seconds,\n",
    "            \"collect_experience/num_seconds_per_batch_step\": num_seconds / self.collect_num_batch_steps,\n",
    "            \"collect_experience/num_env_steps_per_second\": self.collect_num_batch_steps * num_parallel_envs / num_seconds,\n",
    "            \"collect_experience/num_seconds_per_env_step\": num_seconds / (self.collect_num_batch_steps * num_parallel_envs),\n",
    "            \"collect_experience/num_parallel_envs\": float(num_parallel_envs),\n",
    "        }\n",
    "        pl_module.log_dict(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpointWithActorOnnxExport(lightning.pytorch.callbacks.ModelCheckpoint):\n",
    "    def __init__(\n",
    "        self,\n",
    "        example_input: torch.Tensor,\n",
    "        export_kwargs: dict[str, Any] | None = None,\n",
    "        checkpointing_kwargs: dict[str, Any] | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__(**checkpointing_kwargs)\n",
    "        self.example_input = example_input\n",
    "        self.export_kwargs = {} if export_kwargs is None else export_kwargs\n",
    "\n",
    "    def _actor_path(self, checkpoint_path: pathlib.Path | str) -> pathlib.Path:\n",
    "        return pathlib.Path(checkpoint_path).with_suffix(\".actor.onnx\")\n",
    "\n",
    "    def _export_actor(self, ppo: PPO, checkpoint_path: pathlib.Path | str) -> None:\n",
    "        onnx_path = self._actor_path(checkpoint_path)\n",
    "        if not onnx_path.exists():\n",
    "            param = next(ppo.actor.parameters())\n",
    "            inputs = self.example_input.to(param)\n",
    "            torch.onnx.export(ppo.actor, inputs, onnx_path, **self.export_kwargs)\n",
    "\n",
    "    def _save_checkpoint(self, trainer: lightning.Trainer, filepath):\n",
    "        \"\"\"\n",
    "        Save actor onnx checkpoint if the full checkpoint is saved.\n",
    "        \"\"\"\n",
    "        output = super()._save_checkpoint(trainer, filepath)\n",
    "        filepath = pathlib.Path(filepath)\n",
    "        if filepath.exists():\n",
    "            self._export_actor(trainer.model, filepath)\n",
    "        return output\n",
    "\n",
    "    def _remove_checkpoint(self, trainer: lightning.Trainer, filepath):\n",
    "        \"\"\"\n",
    "        Remove actor onnx checkpoint if the full checkpoint is removed.\n",
    "        \"\"\"\n",
    "        output = super()._remove_checkpoint(trainer, filepath)\n",
    "        filepath = pathlib.Path(filepath)\n",
    "        if not filepath.exists():\n",
    "            onnx_path = self._actor_path(filepath)\n",
    "            onnx_path.unlink(missing_ok=True)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "max_steps = -1\n",
    "accelerator = \"cpu\"\n",
    "precision = \"16-mixed\"\n",
    "devices = []\n",
    "\n",
    "optimizer_class = \"AdamW\"\n",
    "learning_rate = 1e-4\n",
    "batch_size = 128\n",
    "\n",
    "hidden_dim = 512\n",
    "num_blocks = 4\n",
    "\n",
    "discount_factor = 0.99\n",
    "importance_sampling_clip_range = (0.8, 1.2)\n",
    "\n",
    "buffer_capacity = 100_000\n",
    "collect_data_num_parallel_envs = 10\n",
    "collect_data_every_n_updates = 10\n",
    "collect_data_num_steps = 20\n",
    "\n",
    "val_check_interval = 100\n",
    "valid_num_runs = 10\n",
    "valid_num_parallel_envs = 10\n",
    "\n",
    "checkpoint_monitor = (\"score/avg\", \"max\")\n",
    "\n",
    "hardcore = False\n",
    "\n",
    "use_early_stopping = False\n",
    "early_stopping_threshold = None\n",
    "early_stopping_patience = 5\n",
    "early_stopping_min_delta = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_devices = \"auto\" if accelerator == \"cpu\" else devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hardcore:\n",
    "    make_env = functools.partial(make_bipedal_walker_env, hardcore=True)\n",
    "else:\n",
    "    make_env = functools.partial(make_bipedal_walker_env, hardcore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: gym.Env = make_env()\n",
    "state, _ = env.reset()\n",
    "example_input_for_onnx = torch.from_numpy(state).float().unsqueeze(0)\n",
    "input_dim = torch.Size(env.observation_space.shape).numel()\n",
    "output_dim = torch.Size(env.action_space.shape).numel()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_config = dict(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_blocks=num_blocks,\n",
    "    activation=torch.nn.GELU(),\n",
    "    use_skip=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./wandb\", exist_ok=True)\n",
    "os.makedirs(\"./videos\", exist_ok=True)\n",
    "\n",
    "hardcore_str = \"hardcore_on\" if hardcore else \"hardcore_off\"\n",
    "\n",
    "wandb_logger = lightning.pytorch.loggers.WandbLogger(\n",
    "    project=\"jku-deep-rl_ppo\",\n",
    "    group=hardcore_str,\n",
    "    tags=[\"ppo\", hardcore_str],\n",
    "    save_dir=\"./wandb\",\n",
    ")\n",
    "wandb_experiment: wandb.wandb_run.Run = wandb_logger.experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(\n",
    "    backbone_config=backbone_config,\n",
    "    output_dim=output_dim,\n",
    "    optimizer_class=optimizer_class,\n",
    "    optimizer_config=dict(lr=learning_rate),\n",
    "    discount=discount_factor,\n",
    "    make_env=make_env,\n",
    "    video_root_folder = f\"./videos/{wandb_experiment.name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(capacity=buffer_capacity, seed=42)\n",
    "\n",
    "experience_generator = DataGeneratorParallel(\n",
    "    agent = ppo,\n",
    "    make_env = make_env,\n",
    "    make_env_kwargs = [{} for _ in range(collect_data_num_parallel_envs)],\n",
    "    num_parallel_envs = collect_data_num_parallel_envs,\n",
    ")\n",
    "\n",
    "experience_collection_callback = DataCollectionCallback(\n",
    "    parallel_experience_generator = experience_generator,\n",
    "    buffer = buffer,\n",
    "    collect_every_n_updates = collect_data_every_n_updates,\n",
    "    collect_num_steps_in_total = collect_data_num_steps,\n",
    ")\n",
    "\n",
    "monitor_metric, metric_mode = checkpoint_monitor\n",
    "checkpointing = ModelCheckpointWithActorOnnxExport(\n",
    "    example_input=example_input_for_onnx,\n",
    "    export_kwargs=dict(\n",
    "        export_params=True,\n",
    "        opset_version=10,\n",
    "        do_constant_folding=True,\n",
    "    ),\n",
    "    checkpointing_kwargs=dict(\n",
    "        monitor=monitor_metric,\n",
    "        mode=metric_mode,\n",
    "        save_last=True,\n",
    "        save_top_k=10,\n",
    "        dirpath=f\"checkpoints/{wandb_logger.experiment.name}/\",\n",
    "        filename=\"step={step}_score_avg={score/avg:.2f}\",\n",
    "        auto_insert_metric_name=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    experience_collection_callback,\n",
    "    checkpointing,\n",
    "]\n",
    "\n",
    "if use_early_stopping:\n",
    "    early_stopping = lightning.pytorch.callbacks.EarlyStopping(\n",
    "        monitor=monitor_metric,\n",
    "        mode=metric_mode,\n",
    "        patience=early_stopping_patience,\n",
    "        min_delta=early_stopping_min_delta,\n",
    "        stopping_threshold=early_stopping_threshold,\n",
    "    )\n",
    "    callbacks.append(early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_num_batches = math.ceil(valid_num_runs / valid_num_parallel_envs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    buffer,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    shuffle=False, # ReplayBuffer already samples randomly\n",
    "    prefetch_factor=32,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "valid_make_env_kwargs = [\n",
    "    {\n",
    "        \"seed\": env_idx * valid_num_batches + batch_ids,\n",
    "    }\n",
    "    for env_idx in range(valid_num_parallel_envs) for batch_ids in range(valid_num_batches)\n",
    "]\n",
    "\n",
    "# validation loader just gives parameters for creating the gym env\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_make_env_kwargs,\n",
    "    batch_size=valid_num_parallel_envs,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=lambda x: x,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger.experiment.config.update({\n",
    "    \"buffer_capacity\": buffer_capacity,\n",
    "    \"collect_data\": {\n",
    "        \"num_parallel_envs\": collect_data_num_parallel_envs,\n",
    "        \"every_n_updates\": collect_data_every_n_updates,\n",
    "        \"collect_data_num_steps\": collect_data_num_steps,\n",
    "    },\n",
    "    \"checkpointing\": {\n",
    "        \"monitor\": checkpointing.monitor,\n",
    "        \"mode\": checkpointing.mode,\n",
    "        \"save_last\": checkpointing.save_last,\n",
    "        \"save_top_k\": checkpointing.save_top_k,\n",
    "    },\n",
    "    \"train_loader\": {\n",
    "        \"batch_size\": train_loader.batch_size,\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"num_parallel_envs\": valid_num_parallel_envs,\n",
    "        \"make_env_kwargs\": valid_make_env_kwargs,\n",
    "        \"total_runs\": len(valid_make_env_kwargs),\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = lightning.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=-1,\n",
    "    max_steps=max_steps,\n",
    "    val_check_interval=val_check_interval,\n",
    "    devices=trainer_devices,\n",
    "    precision=precision,\n",
    "    accelerator=accelerator,\n",
    "    callbacks=callbacks,\n",
    "    num_sanity_val_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    ppo,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=valid_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl-ppo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
