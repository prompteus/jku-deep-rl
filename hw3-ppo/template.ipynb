{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1CumJu-KrCvU"
      },
      "source": [
        "# Deep Reinforcement Lerning Lectures - Policy Gradients"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q37atdWHrCvV"
      },
      "source": [
        "### Imports and auxiliary settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUTCq3LfrCvX"
      },
      "outputs": [],
      "source": [
        "!apt update\n",
        "!apt install -y xvfb x11-utils python-opengl ffmpeg swig\n",
        "!pip install gymnasium==0.27.1 gymnasium[box2d] pyvirtualdisplay imageio-ffmpeg moviepy==1.0.3\n",
        "!pip install onnx onnx2pytorch==0.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te40-9xZrCvc"
      },
      "outputs": [],
      "source": [
        "# select device\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBwre4vHrCvg",
        "outputId": "f287bbd2-4ff0-4a60-fecf-81a2c29e2c15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7ff3c33a1c60>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Cross Framework library for DL\n",
        "import onnx\n",
        "from onnx2pytorch import ConvertModel\n",
        "\n",
        "# Auxiliary Python imports\n",
        "import os\n",
        "import math\n",
        "import io\n",
        "import base64\n",
        "import random\n",
        "import shutil\n",
        "import copy\n",
        "import glob\n",
        "from time import time, strftime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from time import sleep\n",
        "\n",
        "# Environment import and set logger level to display error only\n",
        "# Environment import and set logger level to display error only\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box\n",
        "from gymnasium import logger as gymlogger\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "gymlogger.set_level(gym.logger.ERROR)\n",
        "\n",
        "# Plotting and notebook imports\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "from IPython.display import HTML, display, clear_output\n",
        "\n",
        "# start virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "pydisplay = Display(visible=0, size=(640, 480))\n",
        "pydisplay.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmUL9JubrCvj"
      },
      "outputs": [],
      "source": [
        "# use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugq-0cVvrCvo"
      },
      "source": [
        "### Auxiliary Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSOg1hWi6men"
      },
      "outputs": [],
      "source": [
        "class Logger(object):\n",
        "    \"\"\"Logger that can be used for debugging different values\n",
        "    \"\"\"\n",
        "    def __init__(self, logdir, params=None, debug=False):\n",
        "        self.gradients = []\n",
        "        self.debug = debug\n",
        "        self.basepath = os.path.join(logdir, strftime(\"%Y-%m-%dT%H-%M-%S\"))\n",
        "        os.makedirs(self.basepath, exist_ok=True)\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "        if params is not None and os.path.exists(params):\n",
        "            shutil.copyfile(params, os.path.join(self.basepath, \"params.pkl\"))\n",
        "        self.log_dict = {}\n",
        "        self.dump_idx = {}\n",
        "\n",
        "    def add_gradients(self, grad):\n",
        "        if not self.debug: return\n",
        "        self.gradients.append(grad)\n",
        "\n",
        "    def compute_gradient_variance(self):\n",
        "        vars_ = []\n",
        "        grads_list = [np.zeros_like(self.gradients[0])] * 100\n",
        "        for i, grads in enumerate(self.gradients):\n",
        "            grads_list.append(grads)\n",
        "            grads_list = grads_list[1:]\n",
        "            grad_arr = np.stack(grads_list, axis=0)\n",
        "            g = np.apply_along_axis(grad_variance, axis=-1, arr=grad_arr)\n",
        "            vars_.append(np.mean(g))\n",
        "        return vars_\n",
        "\n",
        "    @property\n",
        "    def param_file(self):\n",
        "        return os.path.join(self.basepath, \"params.pkl\")\n",
        "\n",
        "    @property\n",
        "    def onnx_file(self):\n",
        "        return os.path.join(self.basepath, \"model.onnx\")\n",
        "\n",
        "    @property\n",
        "    def video_dir(self):\n",
        "        return os.path.join(self.basepath, \"videos\")\n",
        "    \n",
        "    @property\n",
        "    def log_dir(self):\n",
        "        return os.path.join(self.basepath, \"logs\")\n",
        "\n",
        "    def log(self, name, value):\n",
        "        if name not in self.log_dict:\n",
        "            self.log_dict[name] = []\n",
        "            self.dump_idx[name] = -1\n",
        "        self.log_dict[name].append((len(self.log_dict[name]), time(), value))\n",
        "    \n",
        "    def get_values(self, name):\n",
        "        if name in self.log_dict:\n",
        "            return [x[2] for x in self.log_dict[name]]\n",
        "        return None\n",
        "    \n",
        "    def dump(self):\n",
        "        for name, rows in self.log_dict.items():\n",
        "            with open(os.path.join(self.log_dir, name + \".log\"), \"a\") as f:\n",
        "                for i, row in enumerate(rows):\n",
        "                    if i > self.dump_idx[name]:\n",
        "                        f.write(\",\".join([str(x) for x in row]) + \"\\n\")\n",
        "                        self.dump_idx[name] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd0Ue1cirCvp"
      },
      "outputs": [],
      "source": [
        "def wrap_env(env, logger, capture_video=True):\n",
        "    # wrapper for recording\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    if capture_video:\n",
        "        env = gym.wrappers.RecordVideo(env, logger.video_dir, episode_trigger=lambda idx: True)\n",
        "    return env\n",
        "\n",
        "\n",
        "def create_env(logger, env_id='BipedalWalker-v3', hardcore=False, capture_video=True):\n",
        "    # initialize environment\n",
        "    env = wrap_env(gym.make(env_id, max_episode_steps=400, hardcore=hardcore, render_mode=\"rgb_array\"), \n",
        "                   logger=logger,\n",
        "                   capture_video=capture_video)\n",
        "    action_size = env.action_space.shape[0]\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    return env, action_size, state_size\n",
        "\n",
        "\n",
        "def set_seed(env, seed=None):\n",
        "    # seeding the envrionment\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def transforms(state):\n",
        "    # transofrm to numpy to tensor and push to device\n",
        "    return torch.FloatTensor(state).to(device)\n",
        "\n",
        "            \n",
        "def test_environment(env, agent=None, seed=42, n_steps=200):\n",
        "    # run and evaluate in the environment\n",
        "    state, info = env.reset(seed=seed)\n",
        "    for i in range(n_steps):\n",
        "        env.render()\n",
        "\n",
        "        if agent is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action, _ = agent.act(state)\n",
        "            action = action.squeeze().cpu().numpy()\n",
        "        state, reward, done, truncated, info = env.step(action)\n",
        "        if done:\n",
        "            state, info = env.reset(seed=seed)\n",
        "    env.close()\n",
        "\n",
        "\n",
        "def get_running_stat(stat, stat_len):\n",
        "    # evaluate stats\n",
        "    cum_sum = np.cumsum(np.insert(stat, 0, 0)) \n",
        "    return (cum_sum[stat_len:] - cum_sum[:-stat_len]) / stat_len\n",
        "\n",
        "\n",
        "def plot_results(runner):\n",
        "    # plot stats\n",
        "    episode, r, l = np.array(runner.stats_rewards_list).T\n",
        "    cum_r = get_running_stat(r, 10)\n",
        "    cum_l = get_running_stat(l, 10)\n",
        "    \n",
        "    plt.figure(figsize=(16, 16))\n",
        "\n",
        "    plt.subplot(321)\n",
        "\n",
        "    # plot rewards\n",
        "    plt.plot(episode[-len(cum_r):], cum_r)\n",
        "    plt.plot(episode, r, alpha=0.5)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Episode Reward')\n",
        "   \n",
        "    plt.subplot(322)\n",
        "    \n",
        "    # plot episode lengths\n",
        "    plt.plot(episode[-len(cum_l):], cum_l)\n",
        "    plt.plot(episode, l, alpha=0.5)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Episode Length')\n",
        "    \n",
        "    plt.subplot(323)\n",
        "    \n",
        "    # plot return\n",
        "    all_returns = np.array(runner.buffer.all_returns)\n",
        "    plt.scatter(range(0, len(all_returns)), all_returns, alpha=0.5)\n",
        "    mean_returns = np.array(runner.buffer.mean_returns)\n",
        "    plt.plot(range(0, len(mean_returns)), mean_returns, color=\"orange\")\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Return')\n",
        "    \n",
        "    plt.subplot(324)\n",
        "    \n",
        "    # plot entropy\n",
        "    entropy_arr = np.array(runner.stats_entropy_list)\n",
        "    plt.plot(range(0, len(entropy_arr)), entropy_arr)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Entropy')\n",
        "\n",
        "    plt.subplot(325)\n",
        "\n",
        "    if runner.logger.debug:\n",
        "        # plot variance\n",
        "        variance_arr = np.array(runner.logger.compute_gradient_variance())\n",
        "        plt.plot(range(0, len(variance_arr)), variance_arr)\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Variance')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "\"\"\"\n",
        "def show_video(logger):\n",
        "    print(logger.video_dir)\n",
        "    mp4list = glob.glob(f'{logger.video_dir}/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                  loop controls style=\"height: 400px;\">\n",
        "                  <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def grad_variance(g):\n",
        "    # compute gradient variance\n",
        "    return np.mean(g**2) - np.mean(g)**2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v9aGjeWvrCvt"
      },
      "source": [
        "### Test environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnk4dSKurCvu"
      },
      "source": [
        "OpenAI offers a set of environments for Reinforcement Learning, which are accessible via the `gymnasium` pip package.\n",
        "In this exercise we will focus on discrete control tasks using a Box2D simulation known as BipedalWalker-v3.\n",
        "To access the Box2D packages, we use the gymnasium pypi `box2d` option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzQtckrZrCvw"
      },
      "outputs": [],
      "source": [
        "# show behavior in envrionment with random agent\n",
        "logger = Logger(\"logdir\")\n",
        "env, _, _ = create_env(logger)\n",
        "test_environment(env)\n",
        "show_video(logger)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TLuXOizAFhfm"
      },
      "source": [
        "### Training Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-IHrVviFcqw"
      },
      "outputs": [],
      "source": [
        "class Transition(object):\n",
        "    \"\"\"Transition helper object\n",
        "    \"\"\"\n",
        "    def __init__(self, state, action, reward, next_state, log_probs):\n",
        "        self.state = state\n",
        "        self.action = action\n",
        "        self.reward = reward\n",
        "        self.next_state = next_state\n",
        "        self.g_return = 0.0\n",
        "        self.log_probs = log_probs\n",
        "\n",
        "\n",
        "class Episode(object):\n",
        "    \"\"\"Class for collecting an episode of transitions\n",
        "    \"\"\"\n",
        "    def __init__(self, discount):\n",
        "        self.discount = discount\n",
        "        self._empty()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def _empty(self):\n",
        "        self.n = 0\n",
        "        self.transitions = []\n",
        "\n",
        "    def reset(self):\n",
        "        self._empty()\n",
        "\n",
        "    def size(self):\n",
        "        return self.n\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.transitions.append(transition)\n",
        "        self.n += 1\n",
        "        \n",
        "    def states(self):\n",
        "        return [s.state for s in self.transitions]\n",
        "    \n",
        "    def actions(self):\n",
        "        return [a.action for a in self.transitions]\n",
        "    \n",
        "    def rewards(self):\n",
        "        return [r.reward for r in self.transitions]\n",
        "\n",
        "    def next_states(self):\n",
        "        return [s_.next_state for s_ in self.transitions]\n",
        "    \n",
        "    def returns(self):\n",
        "        return [r.g_return for r in self.transitions]\n",
        "    \n",
        "    def calculate_return(self):\n",
        "        # calculate the return of the episode\n",
        "        rewards = self.rewards()\n",
        "        trajectory_len = len(rewards)\n",
        "        return_array = torch.zeros((trajectory_len,))\n",
        "        g_return = 0.\n",
        "        for i in range(trajectory_len-1, -1, -1):\n",
        "            g_return = rewards[i] + self.discount*g_return\n",
        "            return_array[i] = g_return\n",
        "            self.transitions[i].g_return = g_return\n",
        "        return return_array\n",
        "\n",
        "\n",
        "class BufferDataset(Dataset):\n",
        "    \"\"\"Buffer dataset used to iterate over buffer samples when training.\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        t = self.data[idx]\n",
        "        return t.state, t.action, t.reward, t.next_state, t.log_probs\n",
        "\n",
        "\n",
        "class RolloutBuffer(object):\n",
        "    # ===================================================\n",
        "    # ++++++++++++++++++++ OPTIONAL +++++++++++++++++++++\n",
        "    # ===================================================\n",
        "    # > feel free to optimize sampling and buffer handling\n",
        "    # ===================================================\n",
        "    \"\"\"Buffer to collect samples while rolling out in the envrionment.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity, batch_size, min_transitions):\n",
        "        self.capacity = capacity\n",
        "        self.batch_size = batch_size\n",
        "        self.min_transitions = min_transitions\n",
        "        self.buffer = []\n",
        "        self._empty()\n",
        "        self.mean_returns = []\n",
        "        self.all_returns = []\n",
        "        \n",
        "    def _empty(self):\n",
        "        # empty the buffer\n",
        "        del self.buffer[:]\n",
        "        self.position = 0\n",
        "\n",
        "    def add(self, episode):\n",
        "        # Saves a transition\n",
        "        episode.calculate_return()\n",
        "        for t in episode.transitions:\n",
        "            if len(self.buffer) < self.capacity:\n",
        "                self.buffer.append(None)\n",
        "            self.buffer[self.position] = t\n",
        "            self.position = (self.position + 1) % self.capacity\n",
        "            \n",
        "    def update_stats(self):\n",
        "        # update the statistics on the buffer\n",
        "        returns = [t.g_return for t in self.buffer]\n",
        "        self.all_returns += returns\n",
        "        mean_return = np.mean(np.array(returns))\n",
        "        self.mean_returns += ([mean_return]*len(returns))\n",
        "\n",
        "    def reset(self):\n",
        "        # calls empty\n",
        "        self._empty()\n",
        "\n",
        "    def create_dataloader(self):\n",
        "        # creates a dataloader for training\n",
        "        train_loader = DataLoader(\n",
        "            BufferDataset(self.buffer),\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qDtLH0LcrCvz"
      },
      "source": [
        "### Define Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4nI-GHnHCtT"
      },
      "outputs": [],
      "source": [
        "class ActorNet(nn.Module):\n",
        "    \"\"\"Actor network (policy)\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size, hidden_size):\n",
        "        super(ActorNet, self).__init__()\n",
        "        # ===================================================\n",
        "        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n",
        "        # ===================================================\n",
        "        # 1) find a suitable architecture for the actor\n",
        "        # ===================================================\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ===================================================\n",
        "        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n",
        "        # ===================================================\n",
        "        # 1) compute mu and sigma logit estimates\n",
        "        # ===================================================\n",
        "        return mu_logits, sigma_logits # must return this!\n",
        "\n",
        "\n",
        "class CriticNet(nn.Module):\n",
        "    \"\"\"Critic network computing the state value\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size, hidden_size):\n",
        "        super(CriticNet, self).__init__()\n",
        "        # ===================================================\n",
        "        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n",
        "        # ===================================================\n",
        "        # 1) find a suitable architecture for the critic\n",
        "        # ===================================================\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ===================================================\n",
        "        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n",
        "        # ===================================================\n",
        "        # 1) compute the value estimates\n",
        "        # ===================================================\n",
        "        pass\n",
        "\n",
        "\n",
        "class ActorCriticNet(nn.Module):\n",
        "    \"\"\"Combining both networks and add helper methods to act and evaluate samples.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size, hidden_size):\n",
        "        super(ActorCriticNet, self).__init__()\n",
        "        # ===================================================\n",
        "        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n",
        "        # ===================================================\n",
        "        # 1) initialize your networks\n",
        "        # ===================================================\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(1, -1)\n",
        "        return self.act(x)\n",
        "\n",
        "    def act(self, state):\n",
        "        # ===================================================\n",
        "        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n",
        "        # ===================================================\n",
        "        # 1) get mean and sigma logits\n",
        "        # 2) softplus the sigma logits to ensure only positive values (add some eps value to avoid 0 probabilities)\n",
        "        # 3) create a Normal distribution based on mu and sigma\n",
        "        # 4) sample from the distribution\n",
        "        # 5) get log probabilities \n",
        "        # ===================================================\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        # ===================================================\n",
        "        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n",
        "        # ===================================================\n",
        "        # 1) get mean and sigma logits\n",
        "        # 2) softplus the sigma logits to ensure only positive values (add some eps value to avoid 0 probabilities)\n",
        "        # 3) create a Normal distribution based on mu and sigma\n",
        "        # 4) get log probabilities \n",
        "        # 5) get entropy from the distribution\n",
        "        # 6) evaluate state value\n",
        "        # ===================================================\n",
        "        pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gqR7Fi06rCv8"
      },
      "source": [
        "### Define Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUNQOvfqrCv9"
      },
      "outputs": [],
      "source": [
        "class Agent(object):\n",
        "    \"\"\"Agent class used for training, saving data and handling the model.\n",
        "    \"\"\"\n",
        "    def __init__(self, buffer, state_size, action_size, hidden_size, learning_rate, logger, eps_clip, n_epochs,\n",
        "                 weight_decay, betas, loss_scales, discount, checkpoint_dir=\"ckpts\"):\n",
        "        self.action_size = action_size\n",
        "        self.state_size = state_size\n",
        "        self.buffer = buffer\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.loss_scales = loss_scales\n",
        "        self.n_epochs = n_epochs\n",
        "        self.eps_clip = eps_clip\n",
        "        self.logger = logger\n",
        "        self.discount = discount\n",
        "        # ===================================================\n",
        "        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n",
        "        # ===================================================\n",
        "        # 1) initialize ActorCriticNet instance\n",
        "        # 2) create initializers\n",
        "        # ===================================================\n",
        "        pass\n",
        "        \n",
        "\n",
        "    def save_checkpoint(self, epoch, info=''):\n",
        "        \"\"\"Saves a model checkpoint\"\"\"\n",
        "        state = {\n",
        "            'info': info,\n",
        "            'epoch': epoch,\n",
        "            'state_dict': self.model.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict()\n",
        "        }\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "        ckp_name = 'best-checkpoint.pth' if info == 'best' else f'checkpoint-epoch{epoch}.pth'\n",
        "        filename = os.path.join(self.checkpoint_dir, ckp_name)\n",
        "        torch.save(state, filename)\n",
        "\n",
        "    def resume_checkpoint(self, resume_path):\n",
        "        \"\"\"Resumes training from an existing model checkpoint\"\"\"\n",
        "        print(\"Loading checkpoint: {} ...\".format(resume_path))\n",
        "        checkpoint = torch.load(resume_path)\n",
        "        # load architecture params from checkpoint.\n",
        "        self.model.load_state_dict(checkpoint['state_dict'])\n",
        "        # load optimizer state from checkpoint only when optimizer type is not changed.\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        print(\"Checkpoint loaded. Resume training\")\n",
        "\n",
        "    def load_onnx_checkpoint(self, onnx_file):\n",
        "        self.model.actor = ConvertModel(onnx.load(onnx_file)).to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def save_onnx_checkpoint(self):\n",
        "        \"\"\"Create an ONNX checkpoint\"\"\"\n",
        "        dummy_input = torch.randn((1, self.state_size))\n",
        "        dummy_input_t = transforms(dummy_input).to(device)\n",
        "        model = agent.model.actor.to(device)\n",
        "        torch.onnx.export(model, dummy_input_t, \"submission_actor.onnx\", verbose=False, opset_version=10, export_params=True, do_constant_folding=True)\n",
        "\n",
        "    def check(self, file_name):\n",
        "        # Load the ONNX model\n",
        "        model = onnx.load(file_name)\n",
        "        # Check that the IR is well formed\n",
        "        onnx.checker.check_model(model)\n",
        "\n",
        "    def act(self, state):\n",
        "        # ===================================================\n",
        "        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n",
        "        # ===================================================\n",
        "        # 1) prepare state tensors\n",
        "        # 2) check if shape is ok or expand acordingly\n",
        "        # 3) get action and log probabilities\n",
        "        # ===================================================\n",
        "        # get action probs then sample from the probabilities\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.n_epochs):   \n",
        "            # create the a dataloader based on the current buffer\n",
        "            loader = self.buffer.create_dataloader()\n",
        "            # iterate over the samples in the dataloader\n",
        "            for states, actions, rewards, next_states, old_log_probs in loader:\n",
        "                # ===================================================\n",
        "                # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n",
        "                # ===================================================\n",
        "                # 1) compute target value with next state and reward\n",
        "                # 2) compute advantage function from target and current state and action\n",
        "                # 3) compute importance sampling ratio from log probabilities\n",
        "                # 4) compute surrogate loss with the advantage and clipped surrogate loss\n",
        "                # 5) compute value losses\n",
        "                # 6) compute total loss with entropy regularization\n",
        "                # 7) compute gradients and perform optimization step\n",
        "                # ===================================================\n",
        "        \n",
        "        # return losses and entropy\n",
        "        return (loss.mean().detach().cpu().numpy(), \n",
        "                value_loss.mean().detach().cpu().numpy(), \n",
        "                policy_loss.mean().detach().cpu().numpy()), entropy.mean().detach().cpu().numpy()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5tPRbfRirCwA"
      },
      "source": [
        "### Define Task Runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgTjRX5FrCwB"
      },
      "outputs": [],
      "source": [
        "class Runner(object):\n",
        "    \"\"\"Runner class performing the rollout in the environment and calling the agent training function periodically\n",
        "    \"\"\"\n",
        "    def __init__(self, env, agent, buffer, logger, discount=0.99, n_episodes=4000, reward_scale=lambda x: x,\n",
        "                 use_buffer_reset=True, stats_interval=1, print_stats=True, min_average_reward=300, render=False, seed=42, checkpoint_interval=100, max_episode_steps=2000):\n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        self.buffer = buffer\n",
        "        self.render = render\n",
        "        self.logger = logger\n",
        "        self.discount = discount\n",
        "        self.n_episodes = n_episodes\n",
        "        self.reward_scale = reward_scale\n",
        "        self.use_buffer_reset = use_buffer_reset\n",
        "        self.stats_interval = stats_interval\n",
        "        self.print_stats = print_stats\n",
        "        self.min_average_reward = min_average_reward\n",
        "        self.seed = seed\n",
        "        self.max_episode_steps = max_episode_steps\n",
        "        # store stats for plotting\n",
        "        self.stats_rewards_list = []\n",
        "        self.stats_entropy_list = []\n",
        "        # stats for running episodes\n",
        "        self.timesteps = 0\n",
        "        self.checkpoint_interval = checkpoint_interval\n",
        "        self.best_model = None\n",
        "        self.max_reward = -np.inf\n",
        "\n",
        "    def run(self):\n",
        "        self.agent.model.eval()\n",
        "        # train for n episodes\n",
        "        with tqdm(range(self.n_episodes)) as pbar:\n",
        "            for e in pbar:\n",
        "                # reset env and stats\n",
        "                state, info = self.env.reset(seed=self.seed)\n",
        "                total_losses = 0.\n",
        "                value_losses = 0.\n",
        "                policy_losses = 0.\n",
        "\n",
        "                # create new episode\n",
        "                episode = Episode(discount=self.discount)\n",
        "\n",
        "                # save model\n",
        "                if e % self.checkpoint_interval == 0:\n",
        "                    self.agent.save_checkpoint(e)\n",
        "\n",
        "                done = False\n",
        "                # train in each episode until episode is done\n",
        "                while not done:\n",
        "                    self.timesteps += 1\n",
        "                    # render env \n",
        "                    if self.render: self.env.render()\n",
        "\n",
        "                    # ===================================================\n",
        "                    # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n",
        "                    # ===================================================\n",
        "                    # 1) get action and log_probs\n",
        "                    # 2) step in the environment to collect transitions\n",
        "                    # 3) uptade agent when buffer has sufficient samples\n",
        "                    # 4) log statistics and reset buffer\n",
        "                    # ===================================================\n",
        "                    \n",
        "                # save best model\n",
        "                if self.best_model is None or episode.total_reward > self.max_reward:\n",
        "                    self.best_model = copy.deepcopy(self.agent.model)\n",
        "                    self.agent.save_checkpoint(e, 'best')\n",
        "                    self.max_reward = episode.total_reward"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YRZITOVcrCwE"
      },
      "source": [
        "### Train Agent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aWQCoVPs7jFv"
      },
      "source": [
        "This is a where you run your actor-critic PPO algorithm.\n",
        "\n",
        "Some tips as to what you need to implement:\n",
        "- chose proper architectures for the actor-critic networks\n",
        "- choose the appropriate loss function (think on which kind of problem you are solving)\n",
        "- choose an optimizer and its hyper-parameters\n",
        "- optional: use a learning-rate scheduler\n",
        "- store your model and training progress often so you don't loose progress if your program crashes\n",
        "\n",
        "In case you use the provided Logger:\n",
        "- `logger.log(\"training_loss\", <loss-value>)` to log a particular value\n",
        "- `logger.dump()` to write the current logs to a log file (e.g. after every episode)\n",
        "- `logger.log_dir`, `logger.param_file`, `logger.onnx_file`, `logger.video_dir` point to files or directories you can use to save files\n",
        "- you might want to specify your google drive folder as a logdir in order to automatically sync your results\n",
        "- if you log the metrics specified in the `plot_metrics` function you can use it to visualize your training progress (or take it as a template to plot your own metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2Bp93eSrCwF"
      },
      "outputs": [],
      "source": [
        "# create environment\n",
        "logger = Logger(\"logdir\")\n",
        "env, action_size, state_size = create_env(logger, capture_video=False)\n",
        "\n",
        "seed = 31\n",
        "# set seed\n",
        "set_seed(env, seed=seed)\n",
        "\n",
        "# ===================================================\n",
        "# ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n",
        "# ===================================================\n",
        "# > find suitable hyperparameters\n",
        "# hyperparameters\n",
        "episodes = ... # run agent for this many episodes\n",
        "epochs = ... # run n epochs of network updates\n",
        "hidden_size = ... # number of units in NN hidden layers\n",
        "learning_rate = ... # learning rate for optimizer\n",
        "discount = ... # discount factor gamma value\n",
        "reward_scale = lambda x: x # reward scaling\n",
        "\n",
        "batch_size = ... # number of samples used for an update\n",
        "min_transitions = ... # number of minimum transitions until update is triggered\n",
        "capacity = ... # maximum number of transitions stored in the rollout buffer\n",
        "\n",
        "use_buffer_reset = True # resets the buffer after every update\n",
        "eps_clip = ... # clipping or importance sampling updates\n",
        "loss_scales = ... # loss scales (value loss, policy loss, entropy loss)\n",
        "betas = ... # optimizer beta parameters\n",
        "weight_decay = ... # optimizer weight decay\n",
        "checkpoint_interval = 100 # checkpoint interval to overwrite the parameters\n",
        "\n",
        "# additional settings\n",
        "print_stats = True\n",
        "render = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjSzAvdRrCwJ"
      },
      "outputs": [],
      "source": [
        "buffer = RolloutBuffer(capacity=capacity, batch_size=batch_size, min_transitions=min_transitions)\n",
        "agent = Agent(buffer=buffer, state_size=state_size, action_size=action_size, hidden_size=hidden_size,\n",
        "              learning_rate=learning_rate, logger=logger, eps_clip=eps_clip, n_epochs=epochs, weight_decay=weight_decay, betas=betas, loss_scales=loss_scales, discount=discount)\n",
        "runner = Runner(env=env, agent=agent, buffer=buffer, logger=logger, discount=discount, n_episodes=episodes, reward_scale=reward_scale, \n",
        "                use_buffer_reset=use_buffer_reset, print_stats=print_stats, render=render, seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcrKd1wdrCwM"
      },
      "outputs": [],
      "source": [
        "runner.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDYzZesLftuD"
      },
      "outputs": [],
      "source": [
        "plot_results(runner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPZIWvHYMcPd"
      },
      "outputs": [],
      "source": [
        "agent.save_onnx_checkpoint()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mJNYxX6dMmj_"
      },
      "source": [
        "## Visualize Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcYYYwtZme8L"
      },
      "outputs": [],
      "source": [
        "# load model from checkpoint\n",
        "agent.resume_checkpoint('ckpts/best-checkpoint.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-hyu2IQrCwU"
      },
      "outputs": [],
      "source": [
        "# run agent in the envrionment\n",
        "env, _, _ = create_env(logger)\n",
        "agent.model.eval()\n",
        "test_environment(env=env, agent=agent)\n",
        "show_video(logger)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qOPIDEs-58o6"
      },
      "source": [
        "# Demo Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR3iGr0p6AvU"
      },
      "outputs": [],
      "source": [
        "agent.load_onnx_checkpoint('demo.onnx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01j00s22JAtz"
      },
      "outputs": [],
      "source": [
        "# run agent in the envrionment\n",
        "logger = Logger('logdir')\n",
        "env, _, _ = create_env(logger)\n",
        "agent.model.eval()\n",
        "test_environment(env=env, agent=agent, n_steps=500)\n",
        "show_video(logger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ki7osku6H0j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
